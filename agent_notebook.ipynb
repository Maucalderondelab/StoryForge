{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc8937f",
   "metadata": {},
   "source": [
    "# Generate the agent\n",
    "This notebook latter we will pass it into the `base agents.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91d46791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import json\n",
    "#Typing\n",
    "from typing import Dict, List, Any, Optional\n",
    "from IPython.display import Image, display\n",
    "\n",
    "#LangGraph/LangChain\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI  # or any other LLM provider\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "#Load env files\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "373f8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a7c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainState(Dict):\n",
    "    messages: Dict[str, Any]\n",
    "    current_fable : str\n",
    "    processing_request : Dict[str, Any]\n",
    "    tool_output : Dict[str, Any]\n",
    "    final_story : str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f8b4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tool(fable aesop)\n",
    "\n",
    "# Cell 2: Add debug prints to aesop_fable_tool\n",
    "def aesop_fable_tool(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tool for processing Aesop's fables using LLM\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Aesop Tool Debug ===\")\n",
    "    current_fable = state.get(\"current_fable\", \"\")\n",
    "    request = state.get(\"processing_request\", {})\n",
    "    action = request.get(\"action\", \"analyze\")\n",
    "    \n",
    "    print(f\"Current fable: {current_fable[:50]}...\")\n",
    "    print(f\"Processing request: {request}\")\n",
    "    print(f\"Action: {action}\")\n",
    "    \n",
    "    if action == \"analyze\":\n",
    "        # Use LLM to analyze the fable\n",
    "        system_prompt = \"\"\"You are an expert in Aesop's fables. Analyze the given fable and provide:\n",
    "        1. The main moral/lesson\n",
    "        2. List of characters (animals/humans)\n",
    "        3. A brief analysis of how the moral is conveyed\n",
    "        4. The story structure (beginning, conflict, resolution)\n",
    "        \n",
    "        Respond in JSON format.\"\"\"\n",
    "        \n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=f\"Analyze this fable:\\n\\n{current_fable}\")\n",
    "        ])\n",
    "        \n",
    "        print(f\"LLM Response: {response.content}\")\n",
    "        \n",
    "        # Parse the response (assuming JSON format)\n",
    "        try:\n",
    "            analysis = json.loads(response.content)\n",
    "        except:\n",
    "            # Fallback if JSON parsing fails\n",
    "            analysis = {\n",
    "                \"moral\": response.content,\n",
    "                \"characters\": [],\n",
    "                \"analysis\": \"\",\n",
    "                \"structure\": \"\"\n",
    "            }\n",
    "            print(\"JSON parsing failed, using fallback\")\n",
    "            \n",
    "        output = {\n",
    "            \"original_fable\": current_fable,\n",
    "            \"analysis\": analysis,\n",
    "            \"word_count\": len(current_fable.split())\n",
    "        }\n",
    "    else:\n",
    "        output = {\"message\": f\"Action {action} not implemented in debug\"}\n",
    "    \n",
    "    print(f\"Tool output: {output}\")\n",
    "    return {\"tool_output\": output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a9a4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Add debug prints to main_agent\n",
    "def main_agent(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main orchestrator agent that uses LLM to decide which tool to use\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Main Agent Debug ===\")\n",
    "    current_fable = state.get(\"current_fable\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    print(f\"Current fable: {current_fable[:50]}...\")\n",
    "    print(f\"Messages: {messages}\")\n",
    "    \n",
    "    # Use LLM to analyze the user's intent and the content\n",
    "    system_prompt = \"\"\"You are an AI orchestrator for a story processing system. \n",
    "    Analyze the user's request and the provided text to determine:\n",
    "    1. Is this an Aesop's fable? (yes/no)\n",
    "    2. What action should be taken? (analyze/retell/expand/modernize/create_new)\n",
    "    3. Any specific requirements? (style, moral, characters)\n",
    "    \n",
    "    Respond in JSON format with keys: is_aesop, action, requirements\"\"\"\n",
    "    \n",
    "    user_message = messages[-1].get(\"content\", \"\") if messages else \"\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=f\"User request: {user_message}\\n\\nText: {current_fable}\")\n",
    "    ])\n",
    "    \n",
    "    print(f\"LLM Response: {response.content}\")\n",
    "    \n",
    "    try:\n",
    "        decision = json.loads(response.content)\n",
    "    except:\n",
    "        # Fallback decision\n",
    "        decision = {\n",
    "            \"is_aesop\": True,\n",
    "            \"action\": \"analyze\",\n",
    "            \"requirements\": {}\n",
    "        }\n",
    "        print(\"JSON parsing failed, using fallback\")\n",
    "    \n",
    "    print(f\"Decision: {decision}\")\n",
    "    \n",
    "    if decision.get(\"is_aesop\", True):\n",
    "        processing_request = {\n",
    "            \"action\": decision.get(\"action\", \"analyze\"),\n",
    "            **decision.get(\"requirements\", {})\n",
    "        }\n",
    "        \n",
    "        result = {\n",
    "            \"processing_request\": processing_request,\n",
    "            \"tool_to_call\": \"aesop_fable_tool\"\n",
    "        }\n",
    "        print(f\"Returning: {result}\")\n",
    "        return result\n",
    "    else:\n",
    "        return {\n",
    "            \"processing_request\": {\"action\": \"analyze\"},\n",
    "            \"tool_to_call\": \"generic_story_tool\"\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5035e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Add debug to tool_router\n",
    "def tool_router(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Routes to the appropriate tool based on main agent's decision\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Tool Router Debug ===\")\n",
    "    print(f\"State keys: {state.keys()}\")\n",
    "    print(f\"Tool to call: {state.get('tool_to_call', 'Not set')}\")\n",
    "    print(f\"Processing request: {state.get('processing_request', {})}\")\n",
    "    \n",
    "    tool_name = state.get(\"tool_to_call\", \"aesop_fable_tool\")\n",
    "    \n",
    "    if tool_name == \"aesop_fable_tool\":\n",
    "        return aesop_fable_tool(state)\n",
    "    else:\n",
    "        return {\"tool_output\": {\"error\": \"Tool not found\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3417e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_output(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the final output for the user using LLM for polish\n",
    "    \"\"\"\n",
    "    tool_output = state.get(\"tool_output\", {})\n",
    "    action = state.get(\"processing_request\", {}).get(\"action\", \"analyze\")\n",
    "    \n",
    "    if \"error\" in tool_output:\n",
    "        final_story = f\"Error: {tool_output['error']}\"\n",
    "    else:\n",
    "        # Use LLM to format the output nicely\n",
    "        system_prompt = f\"\"\"You are a helpful assistant. Format the following {action} output \n",
    "        in a clear, readable way for the user. Make it engaging and well-structured.\"\"\"\n",
    "        \n",
    "        response = llm.invoke([\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=f\"Format this output:\\n{json.dumps(tool_output, indent=2)}\")\n",
    "        ])\n",
    "        \n",
    "        final_story = response.content\n",
    "    \n",
    "    return {\"final_story\": final_story}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "89c7d5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   +-----------+     \n",
      "   | __start__ |     \n",
      "   +-----------+     \n",
      "          *          \n",
      "          *          \n",
      "          *          \n",
      "  +------------+     \n",
      "  | main_agent |     \n",
      "  +------------+     \n",
      "          *          \n",
      "          *          \n",
      "          *          \n",
      "  +-------------+    \n",
      "  | tool_router |    \n",
      "  +-------------+    \n",
      "          *          \n",
      "          *          \n",
      "          *          \n",
      "+-----------------+  \n",
      "| generate_output |  \n",
      "+-----------------+  \n",
      "          *          \n",
      "          *          \n",
      "          *          \n",
      "    +---------+      \n",
      "    | __end__ |      \n",
      "    +---------+      \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MainState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"main_agent\", main_agent)\n",
    "builder.add_node(\"tool_router\", tool_router)\n",
    "builder.add_node(\"generate_output\", generate_final_output)\n",
    "\n",
    "# Logic\n",
    "builder.add_edge(START, \"main_agent\")\n",
    "builder.add_edge(\"main_agent\", \"tool_router\")\n",
    "builder.add_edge(\"tool_router\", \"generate_output\")\n",
    "builder.add_edge(\"generate_output\", END)\n",
    "\n",
    "# Compile\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "# Cell 8: Build and visualize\n",
    "\n",
    "# View\n",
    "print(graph.get_graph().draw_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89aa28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Test the system\n",
    "app = build_story_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51e4ef63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Analysis\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Main Agent Debug ===\n",
      "Current fable: \n",
      "The Fox and the Grapes\n",
      "\n",
      "A hungry Fox saw some fin...\n",
      "Messages: [{'role': 'user', 'content': 'Analyze this fable and tell me about the moral'}]\n",
      "LLM Response: {\n",
      "  \"is_aesop\": \"yes\",\n",
      "  \"action\": \"analyze\",\n",
      "  \"requirements\": {\n",
      "    \"explanation\": \"Explain the moral of the fable.\"\n",
      "  }\n",
      "}\n",
      "Decision: {'is_aesop': 'yes', 'action': 'analyze', 'requirements': {'explanation': 'Explain the moral of the fable.'}}\n",
      "Returning: {'processing_request': {'action': 'analyze', 'explanation': 'Explain the moral of the fable.'}, 'tool_to_call': 'aesop_fable_tool'}\n",
      "\n",
      "=== Tool Router Debug ===\n",
      "State keys: dict_keys(['messages', 'current_fable', 'processing_request', 'tool_output', 'final_story'])\n",
      "Tool to call: Not set\n",
      "Processing request: {'action': 'analyze', 'explanation': 'Explain the moral of the fable.'}\n",
      "\n",
      "=== Aesop Tool Debug ===\n",
      "Current fable: \n",
      "The Fox and the Grapes\n",
      "\n",
      "A hungry Fox saw some fin...\n",
      "Processing request: {'action': 'analyze', 'explanation': 'Explain the moral of the fable.'}\n",
      "Action: analyze\n",
      "LLM Response: ```json\n",
      "{\n",
      "  \"main_moral\": \"It is easy to despise what you cannot have.\",\n",
      "  \"characters\": [\n",
      "    \"Fox\",\n",
      "    \"Grapes\"\n",
      "  ],\n",
      "  \"moral_analysis\": \"The fox tries hard to reach the grapes but fails. Instead of admitting his inability, he convinces himself that the grapes are sour and undesirable. This rationalization shows how people often belittle what they cannot obtain to protect their pride.\",\n",
      "  \"story_structure\": {\n",
      "    \"beginning\": \"A hungry fox sees tempting bunches of grapes hanging high on a vine.\",\n",
      "    \"conflict\": \"The fox tries repeatedly to jump and reach the grapes but fails as they are out of reach.\",\n",
      "    \"resolution\": \"The fox gives up trying and rationalizes that the grapes are sour and not worth having.\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "JSON parsing failed, using fallback\n",
      "Tool output: {'original_fable': '\\nThe Fox and the Grapes\\n\\nA hungry Fox saw some fine bunches of Grapes hanging from a vine that was trained along a high trellis, and did his best to reach them by jumping as high as he could into the air. But it was all in vain, for they were just out of reach: so he gave up trying, and walked away with an air of dignity and unconcern, remarking, \"I thought those Grapes were ripe, but I see now they are quite sour.\"\\n', 'analysis': {'moral': '```json\\n{\\n  \"main_moral\": \"It is easy to despise what you cannot have.\",\\n  \"characters\": [\\n    \"Fox\",\\n    \"Grapes\"\\n  ],\\n  \"moral_analysis\": \"The fox tries hard to reach the grapes but fails. Instead of admitting his inability, he convinces himself that the grapes are sour and undesirable. This rationalization shows how people often belittle what they cannot obtain to protect their pride.\",\\n  \"story_structure\": {\\n    \"beginning\": \"A hungry fox sees tempting bunches of grapes hanging high on a vine.\",\\n    \"conflict\": \"The fox tries repeatedly to jump and reach the grapes but fails as they are out of reach.\",\\n    \"resolution\": \"The fox gives up trying and rationalizes that the grapes are sour and not worth having.\"\\n  }\\n}\\n```', 'characters': [], 'analysis': '', 'structure': ''}, 'word_count': 85}\n",
      "Here is a clear and engaging presentation of the fable and its analysis:\n",
      "\n",
      "---\n",
      "\n",
      "### Original Fable:  \n",
      "**The Fox and the Grapes**\n",
      "\n",
      "A hungry Fox saw some fine bunches of Grapes hanging from a vine that was trained along a high trellis, and did his best to reach them by jumping as high as he could into the air. But it was all in vain, for they were just out of reach: so he gave up trying, and walked away with an air of dignity and unconcern, remarking,  \n",
      "*\"I thought those Grapes were ripe, but I see now they are quite sour.\"*\n",
      "\n",
      "---\n",
      "\n",
      "### Analysis\n",
      "\n",
      "**Main Moral:**  \n",
      "> It is easy to despise what you cannot have.\n",
      "\n",
      "**Characters:**  \n",
      "- Fox  \n",
      "- Grapes\n",
      "\n",
      "**Moral Analysis:**  \n",
      "The fox tries hard to reach the grapes but fails. Instead of admitting his inability, he convinces himself that the grapes are sour and undesirable. This rationalization shows how people often belittle what they cannot obtain to protect their pride.\n",
      "\n",
      "**Story Structure:**  \n",
      "- **Beginning:** A hungry fox sees tempting bunches of grapes hanging high on a vine.  \n",
      "- **Conflict:** The fox tries repeatedly to jump and reach the grapes but fails as they are out of reach.  \n",
      "- **Resolution:** The fox gives up trying and rationalizes that the grapes are sour and not worth having.\n",
      "\n",
      "---\n",
      "\n",
      "**Word Count of the Fable:** 85 words\n",
      "\n",
      "---\n",
      "\n",
      "This fable reminds us about the common human tendency to downplay what we cannot achieve — an insightful lesson wrapped in a simple, timeless tale.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 1: Test Basic Analysis\n",
    "test_fable = \"\"\"\n",
    "The Fox and the Grapes\n",
    "\n",
    "A hungry Fox saw some fine bunches of Grapes hanging from a vine that was trained along a high trellis, and did his best to reach them by jumping as high as he could into the air. But it was all in vain, for they were just out of reach: so he gave up trying, and walked away with an air of dignity and unconcern, remarking, \"I thought those Grapes were ripe, but I see now they are quite sour.\"\n",
    "\"\"\"\n",
    "\n",
    "# Test 1: Basic analysis\n",
    "analysis_state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this fable and tell me about the moral\"}],\n",
    "    \"current_fable\": test_fable,\n",
    "    \"processing_request\": {},\n",
    "    \"tool_output\": {},\n",
    "    \"final_story\": \"\"\n",
    "}\n",
    "\n",
    "print(\"Test 1: Analysis\")\n",
    "print(\"-\" * 50)\n",
    "result = graph.invoke(analysis_state)\n",
    "print(result[\"final_story\"])\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storyforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
