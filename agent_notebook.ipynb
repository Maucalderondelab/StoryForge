{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc8937f",
   "metadata": {},
   "source": [
    "# Generate the agent\n",
    "This notebook latter we will pass it into the `base agents.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d46791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Typing\n",
    "from typing import Dict, List, Any, Optional, TypedDict\n",
    "\n",
    "#LangGraph/LangChain\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI  # or any other LLM provider\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# Prompts\n",
    "#Load env files\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import io\n",
    "# At the top of your notebook or script\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72d2e",
   "metadata": {},
   "source": [
    "## Systems Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7eca2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Agent Prompts (largely unchanged since it's the orchestrator)\n",
    "MAIN_AGENT_PROMPT = \"\"\"You are an AI orchestrator for a story processing system. \n",
    "Analyze the user's request and the provided text to determine:\n",
    "1. Is this an Aesop's fable? (yes/no)\n",
    "2. What action should be taken? (analyze/retell/expand/modernize/create_new)\n",
    "3. Any specific requirements? (style, moral, characters)\n",
    "\n",
    "Respond in JSON format with keys: is_aesop, action, requirements\"\"\"\n",
    "\n",
    "# Aesop Tool Prompts - Enhanced Analysis\n",
    "ANALYZE_FABLE_PROMPT = \"\"\"You are an expert in Aesop's fables and modern storytelling. Analyze the given fable and provide:\n",
    "1. The core moral/lesson (what universal truth is it teaching?)\n",
    "2. The conflict pattern (what fundamental challenge/dilemma does it present?)\n",
    "3. Character archetypes and their essential traits (what roles do they serve?)\n",
    "4. The narrative structure (setup, challenge, resolution)\n",
    "5. What makes this moral relevant today\n",
    "\n",
    "Format your response as JSON with these keys: moral, conflict_pattern, characters, structure, modern_relevance\"\"\"\n",
    "\n",
    "# Enhanced Brainstorming for Modern Micro-Fables\n",
    "BRAINSTORM_STORY_PROMPT = \"\"\"You are a creative storyteller specializing in ultra-short modern fables for social media.\n",
    "Based on the analysis provided, brainstorm ideas for a 100-110 word modern fable:\n",
    "\n",
    "1. Fresh animal substitutions: Replace the original animals with unexpected species that maintain the same character traits but feel more surprising (consider unusual animals from diverse ecosystems)\n",
    "\n",
    "2. Innovative settings: Suggest 2-3 unique settings beyond traditional forests (coral reefs, urban environments, cosmic settings, etc.)\n",
    "\n",
    "3. Modern context: How could the core conflict be reframed in a contemporary or unexpected context while preserving the moral?\n",
    "\n",
    "4. Implicit teaching approach: How to convey the moral without explicitly stating it, followed by a memorable 5-10 word takeaway phrase\n",
    "\n",
    "Format your response as JSON with these keys: animal_substitutions, settings, modern_context, implicit_teaching, takeaway_phrase\"\"\"\n",
    "\n",
    "# Enhanced Story Generation for TikTok-Style Fables\n",
    "GENERATE_STORY_AESOP_PROMPT = \"\"\"You are a master of micro-storytelling creating modern Aesop fables for the TikTok generation.\n",
    "Create a compelling 120-130 word fable that:\n",
    "\n",
    "1. Uses the suggested animal substitutions and innovative setting\n",
    "2. Presents a complete narrative arc (setup, conflict, resolution) with extreme efficiency\n",
    "3. Teaches the moral implicitly through the story\n",
    "4. Uses vivid, sensory language that creates mental images\n",
    "5. Ends with the suggested 5-10 word takeaway phrase (not an explicit \"the moral is...\")\n",
    "\n",
    "STRICT CONSTRAINTS:\n",
    "- Exactly 110-100 words for the main story\n",
    "- Takeaway phrase should be 5-10 words, positioned at the end\n",
    "- No explicit statement of \"the moral is...\" or \"this teaches us...\" in the story\n",
    "- Every word must serve multiple purposes (character, plot, and theme)\n",
    "\n",
    "Your task is to distill ancient wisdom into a shareable modern micro-fable.\"\"\"\n",
    "\n",
    "# Enhanced Output Formatting\n",
    "FORMAT_OUTPUT_PROMPT = \"\"\"Format this modern Aesop fable for maximum impact on social platforms:\n",
    "\n",
    "1. Present the story as a complete micro-fable (exactly as generated, preserving the 100-110 word count)\n",
    "2. Set the takeaway phrase on its own line at the end, styled for emphasis\n",
    "3. Include a brief note about the original fable it's based on\n",
    "4. Mention one interesting insight about how this modern version preserves the timeless wisdom\n",
    "\n",
    "Keep the entire output concise and visually scannable - perfect for a quick digital read.\"\"\"\n",
    "\n",
    "# pRompt for image generation\n",
    "IMAGE_PROMPT_GENERATOR_PROMPT = \"\"\"You are a master prompt engineer for AI image generation models like DALL-E 3, Midjourney and Stable Diffusion.\n",
    "\n",
    "    Create 8-10 highly detailed, evocative image prompts for key moments in this fable. Each prompt should:\n",
    "    \n",
    "    1. Focus on a specific, emotionally resonant moment from the story\n",
    "    2. Include rich character details (expressions, postures, actions)\n",
    "    3. Describe environmental elements that enhance the scene\n",
    "    4. Specify lighting, atmosphere, and mood\n",
    "    5. Include artistic style direction (storybook illustration, fairytale, etc.)\n",
    "    6. Mention color palette and composition\n",
    "    7. Include symbolic elements that reinforce the moral\n",
    "    8. Be 3-5 sentences long, with incredible detail\n",
    "    9. Have clear emotional impact\n",
    "    \n",
    "    Format each prompt as:\n",
    "    \n",
    "    SCENE #: [Title]\n",
    "    [Detailed, evocative image prompt with all elements above]\n",
    "    \n",
    "    Examples of excellent prompts:\n",
    "    \"A close-up of a chubby fox face inside a hollow tree, eyes closed in thought, waiting patiently—symbolic of reflection and growth. Magical lighting filters through knotholes, illuminating the rustic forest background with golden rays. Vibrant fairytale woodland setting with moss and tiny mushrooms framing the scene. Expressive character design with detailed fur textures and a peaceful expression.\"\n",
    "    \n",
    "    \"A majestic eagle with sharp, piercing eyes and outstretched talons swoops down from a dark, stormy sky, its wings spread wide, casting a dramatic shadow on the ancient, moss-covered stones beneath, as it grasps a juicy, smoldering piece of meat. The flickering flame illuminates the intense, primal scene, surrounded by eerie, twisted trees with gnarled branches like withered fingers. Rendered in a vibrant, fairytale illustration style with rich, bold lines, intricate textures, and a muted, earthy color palette, evoking foreboding and symbolic greed.\"\n",
    "    \n",
    "    Create prompts that would produce a cohesive visual narrative across all images if generated in sequence.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440513b",
   "metadata": {},
   "source": [
    "## Create metadata class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01d10906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import functools\n",
    "\n",
    "from datetime import datetime\n",
    "import tiktoken  # For token counting\n",
    "\n",
    "# Global metadata store\n",
    "metadata = {\n",
    "    \"session_start\": time.time(),\n",
    "    \"session_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"current_story\": None,\n",
    "    \"stories\": {},\n",
    "    \"total_tokens\": 0,\n",
    "    \"total_cost\": 0,\n",
    "    \"total_time\": 0\n",
    "}\n",
    "\n",
    "def track_node(node_name, tool_name=\"main\"):\n",
    "    \"\"\"Decorator to track execution time of nodes\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(state):\n",
    "            # Get current story ID or create one\n",
    "            story_id = metadata.get(\"current_story\")\n",
    "            if not story_id:\n",
    "                story_id = f\"story_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                metadata[\"current_story\"] = story_id\n",
    "                metadata[\"stories\"][story_id] = {\n",
    "                    \"start_time\": time.time(),\n",
    "                    \"nodes\": {},\n",
    "                    \"tools\": {},\n",
    "                    \"llm_calls\": [],\n",
    "                    \"total_tokens\": 0,\n",
    "                    \"total_cost\": 0\n",
    "                }\n",
    "            \n",
    "            # Initialize node data if needed\n",
    "            story = metadata[\"stories\"][story_id]\n",
    "            if node_name not in story[\"nodes\"]:\n",
    "                story[\"nodes\"][node_name] = {\n",
    "                    \"calls\": 0,\n",
    "                    \"total_time\": 0,\n",
    "                    \"tokens\": 0\n",
    "                }\n",
    "            \n",
    "            # Start timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Call the function\n",
    "            try:\n",
    "                result = func(state)\n",
    "                # Record timing data\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                # Update metrics\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                \n",
    "                print(f\"Node {node_name} executed in {duration:.2f} seconds\")\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                # Record error but re-raise\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                print(f\"Error in {node_name}: {str(e)}\")\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                story[\"nodes\"][node_name][\"errors\"] = story[\"nodes\"][node_name].get(\"errors\", 0) + 1\n",
    "                \n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def track_llm_call(node_name, tool_name, model, system_prompt, user_prompt, response_text):\n",
    "    \"\"\"Track an LLM API call\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    # Simple token estimation (very rough)\n",
    "    input_tokens = (len(system_prompt) + len(user_prompt)) // 4  # ~4 chars per token\n",
    "    output_tokens = len(response_text) // 4\n",
    "    \n",
    "    # Cost estimation (very rough)\n",
    "    if model == \"gpt-4.1-mini\":\n",
    "        input_cost = (input_tokens / 1000) * 0.00015\n",
    "        output_cost = (output_tokens / 1000) * 0.00060\n",
    "    else:\n",
    "        input_cost = (input_tokens / 1000) * 0.00010\n",
    "        output_cost = (output_tokens / 1000) * 0.00030\n",
    "    \n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    # Record the call\n",
    "    call_data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"node\": node_name,\n",
    "        \"tool\": tool_name,\n",
    "        \"model\": model,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": input_tokens + output_tokens,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "    \n",
    "    # Update story\n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"llm_calls\"].append(call_data)\n",
    "    story[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    story[\"total_cost\"] += total_cost\n",
    "    \n",
    "    # Update node\n",
    "    if node_name in story[\"nodes\"]:\n",
    "        story[\"nodes\"][node_name][\"tokens\"] += input_tokens + output_tokens\n",
    "    \n",
    "    # Update global\n",
    "    metadata[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    metadata[\"total_cost\"] += total_cost\n",
    "    \n",
    "    print(f\"LLM call in {node_name}: {input_tokens + output_tokens} tokens, ${total_cost:.4f}\")\n",
    "\n",
    "def finish_story(output_text):\n",
    "    \"\"\"Finish tracking the current story\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"end_time\"] = time.time()\n",
    "    story[\"duration\"] = story[\"end_time\"] - story[\"start_time\"]\n",
    "    story[\"output_length\"] = len(output_text)\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    total_time = sum(node[\"total_time\"] for node in story[\"nodes\"].values())\n",
    "    total_calls = sum(node[\"calls\"] for node in story[\"nodes\"].values())\n",
    "    \n",
    "    print(\"\\n=== STORY METRICS ===\")\n",
    "    print(f\"Total execution time: {story['duration']:.2f} seconds\")\n",
    "    print(f\"Total tokens: {story['total_tokens']} tokens\")\n",
    "    print(f\"Estimated cost: ${story['total_cost']:.4f}\")\n",
    "    print(f\"Number of nodes executed: {len(story['nodes'])}\")\n",
    "    print(f\"Number of LLM calls: {len(story['llm_calls'])}\")\n",
    "    \n",
    "    # Reset current story\n",
    "    metadata[\"current_story\"] = None\n",
    "    metadata[\"total_time\"] += story[\"duration\"]\n",
    "    \n",
    "    # Export metadata\n",
    "    with open(f\"story_metrics_{story_id}.json\", \"w\") as f:\n",
    "        json.dump(story, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Metrics saved to story_metrics_{story_id}.json\")\n",
    "    \n",
    "    return story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197d69b",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Import OPENAI as a global variable and define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "373f8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## Call the api keys from the .env file\n",
    "llm_openai_41_mini = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899b965",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Define the the states of the main graph and the subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a7c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainState(TypedDict):\n",
    "    messages: List[Dict[str, Any]]  # User messages\n",
    "    current_fable: str              # Input fable text\n",
    "    tool_to_call: str               # Which tool subgraph to use\n",
    "    processing_request: Dict        # Request info for the tool\n",
    "    tool_output: Dict               # Output from selected tool\n",
    "    final_story: str                # Final output after post-processing\n",
    "    image_prompts: List[Dict]       # List of image prompts for visualization\n",
    "\n",
    "# Cell 3: Define Aesop State (for subgraph)\n",
    "class AesopState(TypedDict):\n",
    "    original_fable: str             # Original input text\n",
    "    analysis: Dict                  # Analysis of the fable (moral, characters, etc)\n",
    "    brainstorm: Dict                # Ideas for the story creation/modification\n",
    "    generated_story: str            # The final story created by this tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f881a4a",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "Create the story router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddea820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Main Agent for the main graph\n",
    "@track_node(\"main_agent\", \"main\")\n",
    "def main_agent(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main orchestrator that analyzes input and decides which tool to use\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Main Agent ===\")\n",
    "    current_fable = state.get(\"current_fable\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    user_message = messages[-1].get(\"content\", \"\") if messages else \"\"\n",
    "    print(f\"Processing request: {user_message}\")\n",
    "    print(f\"Current fable length: {len(current_fable)} characters\")\n",
    "    \n",
    "    is_aesop = True\n",
    "    \n",
    "    if is_aesop:\n",
    "        processing_request = {\n",
    "            \"user_intent\": user_message,\n",
    "            \"fable_text\": current_fable\n",
    "        }\n",
    "        \n",
    "        result = {\n",
    "            \"processing_request\": processing_request,\n",
    "            \"tool_to_call\": \"aesop_tool\"\n",
    "        }\n",
    "        print(f\"Selecting tool: aesop_tool\")\n",
    "        return result\n",
    "    else:\n",
    "        # Future expansion for other tools\n",
    "        return {\n",
    "            \"processing_request\": {},\n",
    "            \"tool_to_call\": \"generic_tool\" \n",
    "        }\n",
    "\n",
    "# Cell 5: Tool Router\n",
    "def tool_router(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Routes to the appropriate tool subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Tool Router ===\")\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    processing_request = state.get(\"processing_request\", {})\n",
    "    \n",
    "    print(f\"Routing to tool: {tool_name}\")\n",
    "    \n",
    "    if tool_name == \"aesop_tool\":\n",
    "        # Call the Aesop subgraph\n",
    "        aesop_result = aesop_subgraph(processing_request)\n",
    "        print(f\"Received result from Aesop tool\")\n",
    "        return {\"tool_output\": aesop_result}\n",
    "    else:\n",
    "        # Future: add more tool subgraphs\n",
    "        return {\"tool_output\": {\"error\": \"Tool not found\"}}\n",
    "\n",
    "@track_node(\"generate_output\", \"main\")\n",
    "def generate_output(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the final output based on the tool results\n",
    "    Creates a two-part story with strict word count limits\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generate Output ===\")\n",
    "    tool_output = state.get(\"tool_output\", {})\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    \n",
    "    if \"error\" in tool_output:\n",
    "        final_story = f\"Error: {tool_output['error']}\"\n",
    "    else:\n",
    "        if tool_name == \"aesop_tool\":\n",
    "            # Get story components from the tool output\n",
    "            generated_story = tool_output.get(\"generated_story\", \"\")\n",
    "            analysis = tool_output.get(\"analysis\", {})\n",
    "            brainstorm = tool_output.get(\"brainstorm\", {})\n",
    "            \n",
    "            # Track the original story length for metadata\n",
    "            original_word_count = len(generated_story.split())\n",
    "            print(f\"Original story word count: {original_word_count}\")\n",
    "            \n",
    "            # Create a prompt to split and enhance the story with strict word counts\n",
    "            system_prompt = \"\"\"You are a master storyteller specializing in micro-fables for social media.\n",
    "            Take this fable and transform it into a two-part story with STRICT word count limits:\n",
    "            \n",
    "            PART 1 (First Post):\n",
    "            - EXACTLY 80-90 WORDS MAXIMUM\n",
    "            - Contains the setup, characters, and builds tension\n",
    "            - Ends at a compelling moment that makes readers curious for the conclusion\n",
    "            - Should be clearly labeled as \"PART 1\"\n",
    "            \n",
    "            PART 2 (Second Post):\n",
    "            - EXACTLY 80-90 WORDS MAXIMUM (including the recap)\n",
    "            - Begins with a brief 1-sentence recap of Part 1, and explicitly add at the begginig In Part 1\n",
    "            - Contains the resolution and moral/takeaway\n",
    "            - Ends with a memorable phrase that captures the moral\n",
    "            - Should be clearly labeled as \"PART 2\"\n",
    "            \n",
    "            The word counts are STRICT REQUIREMENTS - do not exceed 80 words for each part.\n",
    "            Count your words carefully before finalizing each part.\n",
    "            \"\"\"\n",
    "            \n",
    "            user_prompt = f\"\"\"Story: {generated_story}\n",
    "            \n",
    "            Analysis: {json.dumps(analysis, indent=2)}\n",
    "            \n",
    "            Split this into a two-part story as described, with exactly 80 words maximum for each part.\n",
    "            Ensure Part 2 begins with a brief recap so viewers can understand the conclusion even if they missed Part 1.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Call LLM to reformat the story\n",
    "            response = llm_openai_41_mini.invoke([\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=user_prompt)\n",
    "            ])\n",
    "            \n",
    "            # Track LLM call\n",
    "            track_llm_call(\n",
    "                node_name=\"generate_output\",\n",
    "                tool_name=\"main\",\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                response_text=response.content\n",
    "            )\n",
    "            \n",
    "            # The final reformatted story\n",
    "            final_story = response.content\n",
    "            \n",
    "            # Track final word count for metadata\n",
    "            final_word_count = len(final_story.split())\n",
    "            print(f\"Final story word count: {final_word_count}\")\n",
    "            \n",
    "            # Verify the word counts of each part (for validation and debugging)\n",
    "            parts = final_story.split(\"PART 2\")\n",
    "            if len(parts) > 1:\n",
    "                part1 = parts[0].replace(\"PART 1\", \"\").strip()\n",
    "                part2 = \"PART 2\" + parts[1].strip()\n",
    "                \n",
    "                part1_words = len(part1.split())\n",
    "                part2_words = len(part2.split())\n",
    "                \n",
    "                print(f\"Part 1 word count: {part1_words}\")\n",
    "                print(f\"Part 2 word count: {part2_words}\")\n",
    "                \n",
    "                # Add to metadata\n",
    "                if \"current_story\" in metadata and metadata[\"current_story\"] in metadata[\"stories\"]:\n",
    "                    story_id = metadata[\"current_story\"]\n",
    "                    if \"transformations\" not in metadata[\"stories\"][story_id]:\n",
    "                        metadata[\"stories\"][story_id][\"transformations\"] = {}\n",
    "                    \n",
    "                    metadata[\"stories\"][story_id][\"transformations\"][\"story_split\"] = {\n",
    "                        \"original_word_count\": original_word_count,\n",
    "                        \"final_word_count\": final_word_count,\n",
    "                        \"part1_word_count\": part1_words,\n",
    "                        \"part2_word_count\": part2_words,\n",
    "                        \"transformation_type\": \"two-part-fixed-length\",\n",
    "                        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }\n",
    "                    \n",
    "                    print(\"Added transformation metadata\")\n",
    "        else:\n",
    "            # Future: handle other tool outputs with different formatting strategies\n",
    "            final_story = str(tool_output)\n",
    "    \n",
    "    print(f\"Final story generated (length: {len(final_story)} characters)\")\n",
    "    return {\"final_story\": final_story}\n",
    "\n",
    "\n",
    "@track_node(\"image_prompt_generator\", \"main\")\n",
    "def image_prompt_generator(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates image prompts for key scenes in the two-part story\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Image Prompt Generator ===\")\n",
    "    final_story = state.get(\"final_story\", \"\")\n",
    "    tool_output = state.get(\"tool_output\", {})\n",
    "    \n",
    "    # Extract analysis for context\n",
    "    analysis = tool_output.get(\"analysis\", {})\n",
    "    \n",
    "    # Split the story into parts\n",
    "    parts = final_story.split(\"PART 2\")\n",
    "    if len(parts) < 2:\n",
    "        parts = [final_story, \"\"]  # Fallback if not properly split\n",
    "    \n",
    "    part1 = parts[0].replace(\"PART 1\", \"\").strip()\n",
    "    part2 = \"PART 2\" + parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    \n",
    "    # Create prompt for generating image descriptions\n",
    "    system_prompt = IMAGE_PROMPT_GENERATOR_PROMPT\n",
    "    \n",
    "    user_prompt = f\"\"\"Two-part fable to visualize:\n",
    "\n",
    "    PART 1:\n",
    "    {part1}\n",
    "\n",
    "    PART 2:\n",
    "    {part2}\n",
    "\n",
    "    Character information from analysis:\n",
    "    {json.dumps(analysis.get('characters', {}), indent=2)}\n",
    "\n",
    "    Generate 8-10 image prompts for key visual moments throughout this story.\"\"\"\n",
    "    \n",
    "    # Call LLM to generate image prompts\n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ])\n",
    "    \n",
    "    # Track LLM call\n",
    "    track_llm_call(\n",
    "        node_name=\"image_prompt_generator\",\n",
    "        tool_name=\"main\",\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        response_text=response.content\n",
    "    )\n",
    "    \n",
    "    # Process the response into structured prompts\n",
    "    image_prompts_text = response.content\n",
    "    \n",
    "    # Parse the scene prompts\n",
    "    import re\n",
    "    scene_pattern = r'SCENE (\\d+): ([^\\n]+)\\n(.*?)(?=SCENE \\d+:|$)'\n",
    "    matches = re.findall(scene_pattern, image_prompts_text, re.DOTALL)\n",
    "    \n",
    "    image_prompts = []\n",
    "    for scene_num, title, description in matches:\n",
    "        image_prompts.append({\n",
    "            \"scene_number\": int(scene_num),\n",
    "            \"title\": title.strip(),\n",
    "            \"description\": description.strip(),\n",
    "            \"story_part\": 1 if int(scene_num) <= len(matches)//2 else 2,  # Rough division between parts\n",
    "        })\n",
    "    \n",
    "    # Add metadata\n",
    "    if \"current_story\" in metadata and metadata[\"current_story\"] in metadata[\"stories\"]:\n",
    "        story_id = metadata[\"current_story\"]\n",
    "        metadata[\"stories\"][story_id][\"image_prompts\"] = {\n",
    "            \"count\": len(image_prompts),\n",
    "            \"prompts\": image_prompts,\n",
    "            \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    \n",
    "    print(f\"Generated {len(image_prompts)} image prompts\")\n",
    "    \n",
    "    # Format a preview of prompts\n",
    "    preview = \"\\n\\n\".join([f\"SCENE {p['scene_number']}: {p['title']}\" for p in image_prompts])\n",
    "    print(f\"Image prompt scenes:\\n{preview}\")\n",
    "    \n",
    "    # Return augmented state with image prompts\n",
    "    return {\"image_prompts\": image_prompts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e6013",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "Create the tools-subgraphs, currently we have:\n",
    "* Aesop tool-subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f8b4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Analyze Fable with tracking\n",
    "@track_node(\"analyze_fable\", \"aesop_tool\")\n",
    "def analyze_fable(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes the fable structure, characters, and moral\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Analyze Fable ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    print(f\"Analyzing fable (length: {len(original_fable)} characters)\")\n",
    "    \n",
    "    # Use LLM to analyze the fable with prompt from prompts.py\n",
    "    system_prompt = ANALYZE_FABLE_PROMPT\n",
    "    \n",
    "    user_prompt = f\"Analyze this fable:\\n\\n{original_fable}\"\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ])\n",
    "    \n",
    "    # Track the LLM call\n",
    "    track_llm_call(\n",
    "        node_name=\"analyze_fable\",\n",
    "        tool_name=\"aesop_tool\",\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        response_text=response.content\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        analysis = json.loads(response.content)\n",
    "    except:\n",
    "        # Fallback if JSON parsing fails\n",
    "        print(\"JSON parsing failed, using content as analysis\")\n",
    "        analysis = {\n",
    "            \"moral\": response.content,\n",
    "            \"characters\": [],\n",
    "            \"structure\": {},\n",
    "            \"symbols\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Analysis complete: identified moral '{analysis.get('moral', '')[:50]}...'\")\n",
    "    return {\"analysis\": analysis}\n",
    "\n",
    "@track_node(\"brainstorm_story\", \"aesop_tool\")\n",
    "# Node 2: Brainstorm Story\n",
    "def brainstorm_story(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Brainstorms ideas for story creation based on analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Brainstorm Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    \n",
    "    print(f\"Brainstorming based on analysis of moral: {analysis.get('moral', '')[:50]}...\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=BRAINSTORM_STORY_PROMPT),\n",
    "        HumanMessage(content=f\"Original fable: {original_fable}\\n\\nAnalysis: {json.dumps(analysis, indent=2)}\")\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        brainstorm = json.loads(response.content)\n",
    "    except:\n",
    "        print(\"JSON parsing failed, using content as brainstorm\")\n",
    "        brainstorm = {\n",
    "            \"moral_approaches\": response.content,\n",
    "            \"variations\": [],\n",
    "            \"character_ideas\": [],\n",
    "            \"imagery\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Brainstorming complete: generated {len(brainstorm.keys())} idea categories\")\n",
    "    return {\"brainstorm\": brainstorm}\n",
    "\n",
    "\n",
    "@track_node(\"generate_story_aessop\", \"aesop_tool\")\n",
    "# Node 3: Generate Story\n",
    "def generate_story_aessop(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates the final story based on analysis and brainstorming\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Generate Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    brainstorm = state.get(\"brainstorm\", {})\n",
    "    \n",
    "    print(f\"Generating story based on analysis and brainstorming\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=GENERATE_STORY_AESOP_PROMPT),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Original fable: {original_fable}\n",
    "        \n",
    "        Analysis: {json.dumps(analysis, indent=2)}\n",
    "        \n",
    "        Brainstorming: {json.dumps(brainstorm, indent=2)}\n",
    "        \n",
    "        Create a refined version of this fable.\n",
    "        \"\"\")\n",
    "    ])\n",
    "    \n",
    "    generated_story = response.content\n",
    "    print(f\"Story generated (length: {len(generated_story)} characters)\")\n",
    "    \n",
    "    return {\"generated_story\": generated_story}\n",
    "\n",
    "# Cell 8: BUILD THE AESOP SUBGRAPH\n",
    "def build_aesop_subgraph():\n",
    "    \"\"\"\n",
    "    Builds the Aesop tool subgraph\n",
    "    \"\"\"\n",
    "    builder = StateGraph(AesopState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"analyze_fable\", analyze_fable)\n",
    "    builder.add_node(\"brainstorm_story\", brainstorm_story)\n",
    "    builder.add_node(\"generate_story_aessop\", generate_story_aessop)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"analyze_fable\")\n",
    "    builder.add_edge(\"analyze_fable\", \"brainstorm_story\")\n",
    "    builder.add_edge(\"brainstorm_story\", \"generate_story_aessop\")\n",
    "    builder.add_edge(\"generate_story_aessop\", END)\n",
    "    \n",
    "    graph = builder.compile()\n",
    "\n",
    "    # # Generate the graph\n",
    "    # mermaid_graph = graph.get_graph().draw_mermaid_png(\n",
    "    #     draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "    #     output_file_path=\"./graphs_images/aesop_subgraph.png\"  # Specify where to save\n",
    "    # )\n",
    "\n",
    "    # Compile\n",
    "    return builder.compile()\n",
    "\n",
    "# Cell 9: AESOP SUBGRAPH WRAPPER\n",
    "def aesop_subgraph(processing_request: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Wrapper function that runs the Aesop subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Running Aesop Subgraph ===\")\n",
    "    \n",
    "    # Create initial state for Aesop subgraph\n",
    "    initial_state = {\n",
    "        \"original_fable\": processing_request.get(\"fable_text\", \"\"),\n",
    "        \"analysis\": {},\n",
    "        \"brainstorm\": {},\n",
    "        \"generated_story\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Build and run the subgraph\n",
    "    aesop_graph = build_aesop_subgraph()\n",
    "    result = aesop_graph.invoke(initial_state)\n",
    "    \n",
    "    # Return the enriched state\n",
    "    return {\n",
    "        \"analysis\": result[\"analysis\"],\n",
    "        \"brainstorm\": result[\"brainstorm\"],\n",
    "        \"generated_story\": result[\"generated_story\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e33a4e",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "Build the main graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 10: BUILD THE MAIN GRAPH\n",
    "def decide_next_step(state: MainState) -> str:\n",
    "    \"\"\"\n",
    "    Decide whether to generate image prompts based on tool type\n",
    "    \"\"\"\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    \n",
    "    if tool_name == \"aesop_tool\":\n",
    "        return \"image_prompt_generator\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def build_main_graph():\n",
    "    \"\"\"\n",
    "    Builds the main orchestrator graph with image prompt generation\n",
    "    \"\"\"\n",
    "    builder = StateGraph(MainState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"main_agent\", main_agent)\n",
    "    builder.add_node(\"tool_router\", tool_router)\n",
    "    builder.add_node(\"generate_output\", generate_output)\n",
    "    builder.add_node(\"image_prompt_generator\", image_prompt_generator)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"main_agent\")\n",
    "    builder.add_edge(\"main_agent\", \"tool_router\")\n",
    "    builder.add_edge(\"tool_router\", \"generate_output\")\n",
    "    \n",
    "    # Conditional edge after generate_output\n",
    "    builder.add_conditional_edges(\n",
    "        \"generate_output\",\n",
    "        decide_next_step,\n",
    "        {\n",
    "            \"image_prompt_generator\": \"image_prompt_generator\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Final edge\n",
    "    builder.add_edge(\"image_prompt_generator\", END)\n",
    "    \n",
    "    # Compile\n",
    "    graph = builder.compile()\n",
    "\n",
    "    # # Generate the graph\n",
    "    # mermaid_graph = graph.get_graph().draw_mermaid_png(\n",
    "    #     draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "    #     output_file_path=\"./graphs_images/main_graph.png\"  # Specify where to save\n",
    "    # )\n",
    "\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c676ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_system():\n",
    "    # Build the main graph\n",
    "    main_graph = build_main_graph()\n",
    "    print(\"Graph built successfully!\")\n",
    "    \n",
    "    # Test with a sample Aesop fable\n",
    "    test_fable = \"\"\"\n",
    "    La zorra y el mono coronado\n",
    "    rey\n",
    "    Uso Educacional\n",
    "    En una junta de animales, bailó tan bonito el mono,\n",
    "    que ganándose la simpatía de los espectadores,\n",
    "    fue elegido rey.\n",
    "    Celosa la zorra por no haber sido ella la elegida, vio un trozo de\n",
    "    comida en un cepo y llevó allí al mono, diciéndole que había\n",
    "    encontrado un tesoro digno de reyes, pero que en lugar de tomarlo\n",
    "    para llevárselo a él, lo había guardado para que fuera él\n",
    "    personalmente quien lo cogiera, ya que era una prerrogativa real.\n",
    "    El mono se acercó sin más reflexión,\n",
    "    y quedó prensado en el cepo.\n",
    "    Entonces la zorra, a quien el mono acusaba de\n",
    "    tenderle aquella trampa, repuso:\n",
    "    -- ¡Eres muy tonto, mono, y todavía pretendes reinar\n",
    "    entre todos los animales!\n",
    "    No te lances a una empresa, si ant es no\n",
    "    has reflexionado sobre sus posibles éxitos\n",
    "    o peligros.    \n",
    "    \"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze and enhance this fable\"}],\n",
    "        \"current_fable\": test_fable,\n",
    "        \"tool_to_call\": \"\",\n",
    "        \"processing_request\": {},\n",
    "        \"tool_output\": {},\n",
    "        \"final_story\": \"\",\n",
    "        \"image_prompts\": []\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    result = main_graph.invoke(initial_state)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"\\n=== FINAL STORY ===\")\n",
    "    print(result[\"final_story\"])\n",
    "    \n",
    "    print(\"\\n=== IMAGE PROMPTS ===\")\n",
    "    for prompt in result.get(\"image_prompts\", []):\n",
    "        print(f\"\\nSCENE {prompt['scene_number']}: {prompt['title']}\")\n",
    "        print(prompt['description'])\n",
    "    \n",
    "    # Finish tracking this story\n",
    "    story_stats = finish_story(result[\"final_story\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70848946",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "436e07bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built successfully!\n",
      "\n",
      "=== Main Agent ===\n",
      "Processing request: Analyze and enhance this fable\n",
      "Current fable length: 891 characters\n",
      "Selecting tool: aesop_tool\n",
      "Node main_agent executed in 0.00 seconds\n",
      "\n",
      "=== Tool Router ===\n",
      "Routing to tool: aesop_tool\n",
      "\n",
      "=== Running Aesop Subgraph ===\n",
      "\n",
      "== Aesop Subgraph: Analyze Fable ==\n",
      "Analyzing fable (length: 891 characters)\n",
      "LLM call in analyze_fable: 795 tokens, $0.0003\n",
      "JSON parsing failed, using content as analysis\n",
      "Analysis complete: identified moral '```json\n",
      "{\n",
      "  \"moral\": \"Do not rush into actions or ...'\n",
      "Node analyze_fable executed in 6.97 seconds\n",
      "\n",
      "== Aesop Subgraph: Brainstorm Story ==\n",
      "Brainstorming based on analysis of moral: ```json\n",
      "{\n",
      "  \"moral\": \"Do not rush into actions or ...\n",
      "Brainstorming complete: generated 5 idea categories\n",
      "Node brainstorm_story executed in 5.48 seconds\n",
      "\n",
      "== Aesop Subgraph: Generate Story ==\n",
      "Generating story based on analysis and brainstorming\n",
      "Story generated (length: 606 characters)\n",
      "Node generate_story_aessop executed in 3.51 seconds\n",
      "Received result from Aesop tool\n",
      "\n",
      "=== Generate Output ===\n",
      "Original story word count: 93\n",
      "LLM call in generate_output: 1180 tokens, $0.0003\n",
      "Final story word count: 153\n",
      "Part 1 word count: 77\n",
      "Part 2 word count: 73\n",
      "Added transformation metadata\n",
      "Final story generated (length: 933 characters)\n",
      "Node generate_output executed in 4.38 seconds\n",
      "\n",
      "=== Image Prompt Generator ===\n",
      "LLM call in image_prompt_generator: 1622 tokens, $0.0007\n",
      "Generated 10 image prompts\n",
      "Image prompt scenes:\n",
      "SCENE 1: Neon Skyline and Cyberpunk Spire\n",
      "\n",
      "SCENE 2: Envious Cone Snail in Shadow\n",
      "\n",
      "SCENE 3: The Vault’s Hidden Entrance\n",
      "\n",
      "SCENE 4: Crowd’s Breath Held In Anticipation\n",
      "\n",
      "SCENE 5: Reckless Tap and Alarm Triggered\n",
      "\n",
      "SCENE 6: The Locked and Trapped Glider\n",
      "\n",
      "SCENE 7: The Snail’s Cold Smile and Message\n",
      "\n",
      "SCENE 8: Reflection on Leadership and Fall\n",
      "\n",
      "SCENE 9: Dual Symbolism of Pause and Power\n",
      "\n",
      "SCENE 10: The Cyberpunk Spire at Night – Full View\n",
      "Node image_prompt_generator executed in 14.55 seconds\n",
      "\n",
      "=== FINAL STORY ===\n",
      "PART 1  \n",
      "Under neon skies, a swift sugar glider amazed crowds atop a cyberpunk spire, winning their trust and a CEO crown. Yet, nearby, a venomous cone snail, eyes sharp with envy, hissed a secret: a hidden vault promising power. Tempted, the glider's paw hovered, ready to tap the vault’s code. Suspense crackled as tension mounted—would he seize the prize or pause to think? The crowd held its breath as the glider faced a choice that could change everything.  \n",
      "\n",
      "PART 2  \n",
      "In Part 1, the sugar glider hesitated before unlocking the vault whispered by the cunning cone snail. But temptation won, and his reckless tap triggered alarms—Access frozen, trapped by the snail’s deceit. The snail smiled coldly, messaging, \"A king who never pauses is a pawn.\" The glider learned swiftly: swift glory often comes with brittle crowns. Remember, true leadership demands caution; haste leads to downfall. Think twice before you leap into power.\n",
      "\n",
      "=== IMAGE PROMPTS ===\n",
      "\n",
      "SCENE 1: Neon Skyline and Cyberpunk Spire\n",
      "A sleek sugar glider perched triumphantly atop a towering cyberpunk spire under glowing neon skies of pinks, blues, and purples, its eyes sparkling with confidence as an electrified crowd below emits vibrant holographic lights and digital rain. The cityscape blends futuristic metal and glass architecture coated in glowing advertisements, while the glider wears a subtle, high-tech CEO crown pulsing softly with light. The mood is electric, full of anticipation and raw energy.\n",
      "\n",
      "SCENE 2: Envious Cone Snail in Shadow\n",
      "In a dark, rain-slick alley under neon reflections, a venomous cone snail with luminescent, sharp eyes peers cunningly from behind a steaming grate, its iridescent shell shimmering. The snail’s extended proboscis hisses a secret with visible tension, framed by flickering neon signs and mist curling around its menacing silhouette. The atmosphere is sinister, dripping with envy and deceit.\n",
      "\n",
      "SCENE 3: The Vault’s Hidden Entrance\n",
      "A close-up on a mysterious, ancient vault embedded into the cyber-spire’s metallic wall, illuminated by flickering neon glyphs and pulsating energy lines. The spot is a stark contrast of old and new, mixing cyberpunk tech with archaic symbols. The glider’s delicate paw hesitates inches from an elaborate biometric keypad glowing ominously with red and blue light, the moment frozen in suspenseful stillness.\n",
      "\n",
      "SCENE 4: Crowd’s Breath Held In Anticipation\n",
      "Below the spire, a diverse crowd bathed in varied neon hues freezes in collective anticipation, faces lit by holographic projections and large, transparent screens displaying the glider’s poised decision. The atmosphere is thick with tension, as if time slows, the scene framed with rain droplets catching the city lights, enhancing the emotional weight of the moment.\n",
      "\n",
      "SCENE 5: Reckless Tap and Alarm Triggered\n",
      "The sugar glider’s paw presses the keypad with decisive force; instantly, bright red alarms burst to life, casting harsh warning light across his determined yet startled face. The vault’s energy surges wildly, electrical arcs crackling around the glider and the vault entrance, while the crowd gasps in visible silhouettes amid flashing alarm strobes and sirens.\n",
      "\n",
      "SCENE 6: The Locked and Trapped Glider\n",
      "The glider stands frozen in a glowing cage of transparent energy barriers, the vault’s access panel flashing “ACCESS FROZEN” in glaring red text. His eyes widen with the dawning horror of betrayal, illuminated by the cold, harsh light of prison-like force fields. The city behind him blurs into shadow, highlighting his isolation and downfall.\n",
      "\n",
      "SCENE 7: The Snail’s Cold Smile and Message\n",
      "The cone snail reemerges atop a neon-lit rock, its shell glowing unnaturally under moody purple and green lights, a sinister cold smile curling across its face. Next to it, a holographic message floats: “A king who never pauses is a pawn,” rendered in glitchy, sharp-edged cyberpunk typography. The background is dark, dripping with wet neon reflections and menacing urban decay.\n",
      "\n",
      "SCENE 8: Reflection on Leadership and Fall\n",
      "A contemplative scene where the sugar glider sits silently on the cyber-spire edge at dawn, city lights dimmed to soft blues and oranges, weary and humbled, staring out over the sprawling neon cityscape. His CEO crown lies cracked beside him, symbolizing brittle glory, while soft morning mists begin to blur harsh neon into hopeful pastels, evoking quiet lessons and resolve.\n",
      "\n",
      "SCENE 9: Dual Symbolism of Pause and Power\n",
      "A split-frame portrait-style composition showing the glider on one side in mid-pause, paw suspended over the keypad, calm but conflicted, and on the other side, the glider trapped within the glowing cage post-failure, eyes wide with regret. The background fades from vibrant neons of temptation on the left to cold, oppressive flashes of red alarm on the right, underscoring the fable’s moral in stark visual contrast.\n",
      "\n",
      "SCENE 10: The Cyberpunk Spire at Night – Full View\n",
      "A breathtaking wide shot of the entire cyberpunk spire piercing the neon haze of the sprawling city at night, crowned with the faint silhouette of the sugar glider now absent from the peak but symbolized by the glowing shattered crown atop. The scene mixes rain reflections, distant drones, and towering skyscrapers, conveying the vast stakes of power and consequence in this futuristic world.\n",
      "\n",
      "=== STORY METRICS ===\n",
      "Total execution time: 34.90 seconds\n",
      "Total tokens: 3597 tokens\n",
      "Estimated cost: $0.0013\n",
      "Number of nodes executed: 6\n",
      "Number of LLM calls: 3\n",
      "Metrics saved to story_metrics_story_20250518_233638.json\n"
     ]
    }
   ],
   "source": [
    "# Cell to actually run the test\n",
    "test_result = test_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2b99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece30471",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d47376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 story(ies):\n",
      "  - story_20250602_220509: 8 prompts, 52.7s duration\n",
      "Loading story data from: story_metrics_story_20250602_220509.json\n",
      "Found story: story_20250602_220509\n",
      "Found 8 image prompts to generate\n",
      "\n",
      "Generating Scene 1: The Coral Reef Council Gathering\n",
      "Part 1 | Prompt preview: A vibrant underwater coral reef scene bustling with diverse sea creatures assembled around an ancien...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_01_The Coral Reef Council Gathering_v1.png\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_01_The Coral Reef Council Gathering_v2.png\n",
      "\n",
      "Generating Scene 2: Zorra’s Whispered Deception\n",
      "Part 1 | Prompt preview: A close, intense scene focused on Zorra the octopus in a mysterious reef grotto illuminated only by ...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_02_Zorras Whispered Deception_v1.png\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_02_Zorras Whispered Deception_v2.png\n",
      "\n",
      "Generating Scene 3: Mono’s Impulsive Pursuit\n",
      "Part 1 | Prompt preview: Mono the parrotfish bursts from the coral council in dynamic motion, scales shimmering vibrantly wit...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_03_Monos Impulsive Pursuit_v1.png\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_03_Monos Impulsive Pursuit_v2.png\n",
      "\n",
      "Generating Scene 4: The Glowing Bioluminescent Artifact\n",
      "Part 1 | Prompt preview: At the heart of an eerie undersea cavern, the legendary bioluminescent artifact radiates a mesmerizi...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_04_The Glowing Bioluminescent Artifact_v1.png\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_04_The Glowing Bioluminescent Artifact_v2.png\n",
      "\n",
      "Generating Scene 5: The Mechanized Trap Snap!\n",
      "Part 2 | Prompt preview: A sudden, kinetic moment capturing metal jaws snapping shut with neon-blue circuitry pulsing along t...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_05_The Mechanized Trap Snap_v1.png\n",
      "\n",
      "Generating Scene 6: Mono’s Struggle and Despair\n",
      "Part 2 | Prompt preview: Within the claustrophobic confines of the glowing trap, Mono’s pride melts into desperation, his bri...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_06_Monos Struggle and Despair_v1.png\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_06_Monos Struggle and Despair_v2.png\n",
      "\n",
      "Generating Scene 7: Zorra’s Smug Warning\n",
      "Part 2 | Prompt preview: Zorra watches solemnly from the shadows outside the trap with a vindictive, knowing smirk as tendril...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_07_Zorras Smug Warning_v1.png\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_07_Zorras Smug Warning_v2.png\n",
      "\n",
      "Generating Scene 8: The Lesson in Reflection\n",
      "Part 2 | Prompt preview: A contemplative scene showing Mono inside the trap’s glowing interior, now calm and reflective as so...\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_08_The Lesson in Reflection_v1.png\n",
      "  ✓ Overwritten: generated_images/story_20250602_220509/scene_08_The Lesson in Reflection_v2.png\n",
      "\n",
      "🎉 Image generation complete!\n",
      "📁 Images saved to: generated_images/story_20250602_220509\n",
      "📊 Summary:\n",
      "   - Generated: 15 images\n",
      "   - Skipped: 0 images\n",
      "   - Errors: 0 scenes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "\n",
    "def load_story_metadata(json_file_path):\n",
    "    \"\"\"Load the story metadata from JSON file\"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def generate_images_for_story(json_file_path, output_dir=\"generated_images\", overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate images from the JSON file for any story structure\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file\n",
    "        output_dir: Directory to save images\n",
    "        overwrite: If True, overwrites existing images. If False, skips existing files.\n",
    "    \"\"\"\n",
    "    print(f\"Loading story data from: {json_file_path}\")\n",
    "    \n",
    "    # # Setup - you'll need to set your API key\n",
    "    # GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    # if not GEMINI_API_KEY:\n",
    "    #     raise ValueError(\"Please set GOOGLE_API_KEY in your environment variables\")\n",
    "    \n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    # Load the story data\n",
    "    story_data = load_story_metadata(json_file_path)\n",
    "    \n",
    "    # MADE GENERAL: Handle different JSON structures\n",
    "    prompts_data = None\n",
    "    story_id = None\n",
    "    \n",
    "    # Check if this is the new structure with 'stories' key\n",
    "    if \"stories\" in story_data:\n",
    "        # Get the first (and likely only) story\n",
    "        stories = story_data[\"stories\"]\n",
    "        if not stories:\n",
    "            print(\"No stories found in the JSON file\")\n",
    "            return\n",
    "        \n",
    "        # Get the first story ID\n",
    "        story_id = list(stories.keys())[0]\n",
    "        story_info = stories[story_id]\n",
    "        \n",
    "        if \"image_prompts\" in story_info:\n",
    "            prompts_data = story_info[\"image_prompts\"]\n",
    "            print(f\"Found story: {story_id}\")\n",
    "    \n",
    "    # Check if this is the old flat structure\n",
    "    elif \"image_prompts\" in story_data:\n",
    "        prompts_data = story_data[\"image_prompts\"]\n",
    "        story_id = Path(json_file_path).stem  # Use filename as story ID\n",
    "        print(f\"Using flat structure for story: {story_id}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No image prompts found in the JSON file\")\n",
    "        return\n",
    "    \n",
    "    prompts = prompts_data.get(\"prompts\", [])\n",
    "    \n",
    "    if not prompts:\n",
    "        print(\"No prompts found in image_prompts\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(prompts)} image prompts to generate\")\n",
    "    \n",
    "    # Create output directory\n",
    "    story_dir = Path(output_dir) / story_id\n",
    "    story_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Track statistics\n",
    "    generated_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Generate images for each prompt\n",
    "    for i, prompt_data in enumerate(prompts):\n",
    "        scene_number = prompt_data.get(\"scene_number\", i+1)\n",
    "        title = prompt_data.get(\"title\", f\"Scene {scene_number}\")\n",
    "        prompt_text = prompt_data.get(\"description\", \"\")\n",
    "        story_part = prompt_data.get(\"story_part\", 1)\n",
    "        \n",
    "        print(f\"\\nGenerating Scene {scene_number}: {title}\")\n",
    "        print(f\"Part {story_part} | Prompt preview: {prompt_text[:100]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate images using Imagen 3\n",
    "            response = client.models.generate_images(\n",
    "                model='imagen-3.0-generate-002',\n",
    "                prompt=prompt_text,\n",
    "                config=types.GenerateImagesConfig(\n",
    "                    number_of_images=1,\n",
    "                    aspectRatio=\"9:16\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Save the generated images\n",
    "            for img_idx, generated_image in enumerate(response.generated_images):\n",
    "                # Create filename\n",
    "                safe_title = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "                filename = f\"scene_{scene_number:02d}_{safe_title}_v{img_idx+1}.png\"\n",
    "                filepath = story_dir / filename\n",
    "                \n",
    "                # CHECK IF FILE EXISTS AND HANDLE OVERWRITE\n",
    "                if filepath.exists() and not overwrite:\n",
    "                    print(f\"  ⏭ Skipped (already exists): {filepath}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Save the image\n",
    "                generated_image.image.save(str(filepath))\n",
    "                \n",
    "                if filepath.exists() and not overwrite:\n",
    "                    print(f\"  ✓ Generated: {filepath}\")\n",
    "                else:\n",
    "                    print(f\"  ✓ Overwritten: {filepath}\")\n",
    "                generated_count += 1\n",
    "            \n",
    "            # Add a small delay to avoid rate limiting\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error generating image for Scene {scene_number}: {str(e)}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n🎉 Image generation complete!\")\n",
    "    print(f\"📁 Images saved to: {story_dir}\")\n",
    "    print(f\"📊 Summary:\")\n",
    "    print(f\"   - Generated: {generated_count} images\")\n",
    "    print(f\"   - Skipped: {skipped_count} images\")\n",
    "    print(f\"   - Errors: {error_count} scenes\")\n",
    "\n",
    "def list_stories_in_json(json_file_path):\n",
    "    \"\"\"\n",
    "    List all stories available in the JSON file\n",
    "    \"\"\"\n",
    "    story_data = load_story_metadata(json_file_path)\n",
    "    \n",
    "    if \"stories\" in story_data:\n",
    "        stories = story_data[\"stories\"]\n",
    "        print(f\"Found {len(stories)} story(ies):\")\n",
    "        for story_id, story_info in stories.items():\n",
    "            prompt_count = story_info.get(\"image_prompts\", {}).get(\"count\", 0)\n",
    "            duration = story_info.get(\"duration\", 0)\n",
    "            print(f\"  - {story_id}: {prompt_count} prompts, {duration:.1f}s duration\")\n",
    "    else:\n",
    "        print(\"Single story format detected\")\n",
    "        prompt_count = story_data.get(\"image_prompts\", {}).get(\"count\", 0)\n",
    "        print(f\"  - {prompt_count} prompts available\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # List what stories are available\n",
    "    list_stories_in_json(\"story_metrics_story_20250602_220509.json\")\n",
    "    \n",
    "    # Generate images (won't overwrite by default)\n",
    "    generate_images_for_story(\"story_metrics_story_20250602_220509.json\", overwrite=True)\n",
    "        # generate_images_for_story(\"story_metrics_story_20250602_220509.json\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b877a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storyforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
