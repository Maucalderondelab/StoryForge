{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc8937f",
   "metadata": {},
   "source": [
    "# Generate the agent\n",
    "This notebook latter we will pass it into the `base agents.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91d46791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Typing\n",
    "from typing import Dict, List, Any, Optional, TypedDict\n",
    "from IPython.display import Image, display\n",
    "\n",
    "#LangGraph/LangChain\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI  # or any other LLM provider\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# Prompts\n",
    "#Load env files\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72d2e",
   "metadata": {},
   "source": [
    "## Systems Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7eca2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Agent Prompts\n",
    "MAIN_AGENT_PROMPT = \"\"\"You are an AI orchestrator for a story processing system. \n",
    "Analyze the user's request and the provided text to determine:\n",
    "1. Is this an Aesop's fable? (yes/no)\n",
    "2. What action should be taken? (analyze/retell/expand/modernize/create_new)\n",
    "3. Any specific requirements? (style, moral, characters)\n",
    "\n",
    "Respond in JSON format with keys: is_aesop, action, requirements\"\"\"\n",
    "\n",
    "# Aesop Tool Prompts\n",
    "ANALYZE_FABLE_PROMPT = \"\"\"You are an expert in Aesop's fables. Analyze the given fable and provide:\n",
    "1. The main moral/lesson\n",
    "2. List of main characters and their traits\n",
    "3. The story structure (beginning, conflict, resolution)\n",
    "4. Symbols and metaphors used\n",
    "\n",
    "Format your response as JSON with these keys: moral, characters, structure, symbols\"\"\"\n",
    "\n",
    "BRAINSTORM_STORY_PROMPT = \"\"\"You are a creative storyteller specializing in Aesop's fables.\n",
    "Based on the analysis provided, brainstorm ideas for:\n",
    "1. How to preserve or enhance the moral lesson\n",
    "2. Potential variations or twists on the story\n",
    "3. Engaging ways to present the characters\n",
    "4. Vivid imagery and descriptions to include\n",
    "\n",
    "Format your response as JSON with these keys: moral_approaches, variations, character_ideas, imagery\"\"\"\n",
    "\n",
    "GENERATE_STORY_PROMPT = \"\"\"You are a master storyteller in the style of Aesop's fables.\n",
    "Create a compelling fable that:\n",
    "1. Clearly conveys the identified moral lesson\n",
    "2. Features the characters with their essential traits\n",
    "3. Follows a clear narrative structure\n",
    "4. Incorporates vivid imagery from the brainstorming\n",
    "5. Ends with an explicit statement of the moral\n",
    "\n",
    "Write in a clear, engaging style suitable for a wide audience.\"\"\"\n",
    "\n",
    "# Output Generator Prompts\n",
    "FORMAT_OUTPUT_PROMPT = \"\"\"Format this Aesop fable output in a clear, engaging way.\n",
    "Include the moral lesson and any interesting observations from the analysis.\n",
    "Make it suitable for the target audience.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440513b",
   "metadata": {},
   "source": [
    "## Create metadata class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "01d10906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import functools\n",
    "\n",
    "from datetime import datetime\n",
    "import tiktoken  # For token counting\n",
    "\n",
    "# Global metadata store\n",
    "metadata = {\n",
    "    \"session_start\": time.time(),\n",
    "    \"session_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"current_story\": None,\n",
    "    \"stories\": {},\n",
    "    \"total_tokens\": 0,\n",
    "    \"total_cost\": 0,\n",
    "    \"total_time\": 0\n",
    "}\n",
    "\n",
    "def track_node(node_name, tool_name=\"main\"):\n",
    "    \"\"\"Decorator to track execution time of nodes\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(state):\n",
    "            # Get current story ID or create one\n",
    "            story_id = metadata.get(\"current_story\")\n",
    "            if not story_id:\n",
    "                story_id = f\"story_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                metadata[\"current_story\"] = story_id\n",
    "                metadata[\"stories\"][story_id] = {\n",
    "                    \"start_time\": time.time(),\n",
    "                    \"nodes\": {},\n",
    "                    \"tools\": {},\n",
    "                    \"llm_calls\": [],\n",
    "                    \"total_tokens\": 0,\n",
    "                    \"total_cost\": 0\n",
    "                }\n",
    "            \n",
    "            # Initialize node data if needed\n",
    "            story = metadata[\"stories\"][story_id]\n",
    "            if node_name not in story[\"nodes\"]:\n",
    "                story[\"nodes\"][node_name] = {\n",
    "                    \"calls\": 0,\n",
    "                    \"total_time\": 0,\n",
    "                    \"tokens\": 0\n",
    "                }\n",
    "            \n",
    "            # Start timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Call the function\n",
    "            try:\n",
    "                result = func(state)\n",
    "                # Record timing data\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                # Update metrics\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                \n",
    "                print(f\"Node {node_name} executed in {duration:.2f} seconds\")\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                # Record error but re-raise\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                print(f\"Error in {node_name}: {str(e)}\")\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                story[\"nodes\"][node_name][\"errors\"] = story[\"nodes\"][node_name].get(\"errors\", 0) + 1\n",
    "                \n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def track_llm_call(node_name, tool_name, model, system_prompt, user_prompt, response_text):\n",
    "    \"\"\"Track an LLM API call\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    # Simple token estimation (very rough)\n",
    "    input_tokens = (len(system_prompt) + len(user_prompt)) // 4  # ~4 chars per token\n",
    "    output_tokens = len(response_text) // 4\n",
    "    \n",
    "    # Cost estimation (very rough)\n",
    "    if model == \"gpt-4.1-mini\":\n",
    "        input_cost = (input_tokens / 1000) * 0.00015\n",
    "        output_cost = (output_tokens / 1000) * 0.00060\n",
    "    else:\n",
    "        input_cost = (input_tokens / 1000) * 0.00010\n",
    "        output_cost = (output_tokens / 1000) * 0.00030\n",
    "    \n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    # Record the call\n",
    "    call_data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"node\": node_name,\n",
    "        \"tool\": tool_name,\n",
    "        \"model\": model,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": input_tokens + output_tokens,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "    \n",
    "    # Update story\n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"llm_calls\"].append(call_data)\n",
    "    story[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    story[\"total_cost\"] += total_cost\n",
    "    \n",
    "    # Update node\n",
    "    if node_name in story[\"nodes\"]:\n",
    "        story[\"nodes\"][node_name][\"tokens\"] += input_tokens + output_tokens\n",
    "    \n",
    "    # Update global\n",
    "    metadata[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    metadata[\"total_cost\"] += total_cost\n",
    "    \n",
    "    print(f\"LLM call in {node_name}: {input_tokens + output_tokens} tokens, ${total_cost:.4f}\")\n",
    "\n",
    "def finish_story(output_text):\n",
    "    \"\"\"Finish tracking the current story\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"end_time\"] = time.time()\n",
    "    story[\"duration\"] = story[\"end_time\"] - story[\"start_time\"]\n",
    "    story[\"output_length\"] = len(output_text)\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    total_time = sum(node[\"total_time\"] for node in story[\"nodes\"].values())\n",
    "    total_calls = sum(node[\"calls\"] for node in story[\"nodes\"].values())\n",
    "    \n",
    "    print(\"\\n=== STORY METRICS ===\")\n",
    "    print(f\"Total execution time: {story['duration']:.2f} seconds\")\n",
    "    print(f\"Total tokens: {story['total_tokens']} tokens\")\n",
    "    print(f\"Estimated cost: ${story['total_cost']:.4f}\")\n",
    "    print(f\"Number of nodes executed: {len(story['nodes'])}\")\n",
    "    print(f\"Number of LLM calls: {len(story['llm_calls'])}\")\n",
    "    \n",
    "    # Reset current story\n",
    "    metadata[\"current_story\"] = None\n",
    "    metadata[\"total_time\"] += story[\"duration\"]\n",
    "    \n",
    "    # Export metadata\n",
    "    with open(f\"story_metrics_{story_id}.json\", \"w\") as f:\n",
    "        json.dump(story, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Metrics saved to story_metrics_{story_id}.json\")\n",
    "    \n",
    "    return story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197d69b",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Import OPENAI as a global variable and define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "373f8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## Call the api keys from the .env file\n",
    "llm_openai_41_mini = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899b965",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Define the the states of the main graph and the subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1a7c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainState(TypedDict):\n",
    "    messages: List[Dict[str, Any]]  # User messages\n",
    "    current_fable: str              # Input fable text\n",
    "    tool_to_call: str               # Which tool subgraph to use\n",
    "    processing_request: Dict        # Request info for the tool\n",
    "    tool_output: Dict               # Output from selected tool\n",
    "    final_story: str                # Final output after post-processing\n",
    "\n",
    "# Cell 3: Define Aesop State (for subgraph)\n",
    "class AesopState(TypedDict):\n",
    "    original_fable: str             # Original input text\n",
    "    analysis: Dict                  # Analysis of the fable (moral, characters, etc)\n",
    "    brainstorm: Dict                # Ideas for the story creation/modification\n",
    "    generated_story: str            # The final story created by this tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f881a4a",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "Create the story router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ddea820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Main Agent for the main graph\n",
    "@track_node(\"main_agent\", \"main\")\n",
    "def main_agent(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main orchestrator that analyzes input and decides which tool to use\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Main Agent ===\")\n",
    "    current_fable = state.get(\"current_fable\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    user_message = messages[-1].get(\"content\", \"\") if messages else \"\"\n",
    "    print(f\"Processing request: {user_message}\")\n",
    "    print(f\"Current fable length: {len(current_fable)} characters\")\n",
    "    \n",
    "    is_aesop = True\n",
    "    \n",
    "    if is_aesop:\n",
    "        processing_request = {\n",
    "            \"user_intent\": user_message,\n",
    "            \"fable_text\": current_fable\n",
    "        }\n",
    "        \n",
    "        result = {\n",
    "            \"processing_request\": processing_request,\n",
    "            \"tool_to_call\": \"aesop_tool\"\n",
    "        }\n",
    "        print(f\"Selecting tool: aesop_tool\")\n",
    "        return result\n",
    "    else:\n",
    "        # Future expansion for other tools\n",
    "        return {\n",
    "            \"processing_request\": {},\n",
    "            \"tool_to_call\": \"generic_tool\" \n",
    "        }\n",
    "\n",
    "# Cell 5: Tool Router\n",
    "def tool_router(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Routes to the appropriate tool subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Tool Router ===\")\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    processing_request = state.get(\"processing_request\", {})\n",
    "    \n",
    "    print(f\"Routing to tool: {tool_name}\")\n",
    "    \n",
    "    if tool_name == \"aesop_tool\":\n",
    "        # Call the Aesop subgraph\n",
    "        aesop_result = aesop_subgraph(processing_request)\n",
    "        print(f\"Received result from Aesop tool\")\n",
    "        return {\"tool_output\": aesop_result}\n",
    "    else:\n",
    "        # Future: add more tool subgraphs\n",
    "        return {\"tool_output\": {\"error\": \"Tool not found\"}}\n",
    "\n",
    "# Cell 6: Final Output Generator\n",
    "def generate_output(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the final output based on the tool results\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generate Output ===\")\n",
    "    tool_output = state.get(\"tool_output\", {})\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    \n",
    "    if \"error\" in tool_output:\n",
    "        final_story = f\"Error: {tool_output['error']}\"\n",
    "    else:\n",
    "        if tool_name == \"aesop_tool\":\n",
    "            # For Aesop tool, use the generated story\n",
    "            generated_story = tool_output.get(\"generated_story\", \"\")\n",
    "            analysis = tool_output.get(\"analysis\", {})\n",
    "            \n",
    "            \n",
    "            response = llm_openai_41_mini.invoke([\n",
    "                SystemMessage(content=FORMAT_OUTPUT_PROMPT),\n",
    "                HumanMessage(content=f\"Story: {generated_story}\\n\\nAnalysis: {json.dumps(analysis, indent=2)}\")\n",
    "            ])\n",
    "            \n",
    "            final_story = response.content\n",
    "        else:\n",
    "            # Future: handle other tool outputs\n",
    "            final_story = str(tool_output)\n",
    "    \n",
    "    print(f\"Final story generated (length: {len(final_story)} characters)\")\n",
    "    return {\"final_story\": final_story}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e6013",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "Create the tools-subgraphs, currently we have:\n",
    "* Aesop tool-subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f8b4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Analyze Fable with tracking\n",
    "@track_node(\"analyze_fable\", \"aesop_tool\")\n",
    "def analyze_fable(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes the fable structure, characters, and moral\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Analyze Fable ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    print(f\"Analyzing fable (length: {len(original_fable)} characters)\")\n",
    "    \n",
    "    # Use LLM to analyze the fable with prompt from prompts.py\n",
    "    system_prompt = \"\"\"You are an expert in Aesop's fables. Analyze the given fable and provide:\n",
    "    1. The main moral/lesson\n",
    "    2. List of main characters and their traits\n",
    "    3. The story structure (beginning, conflict, resolution)\n",
    "    4. Symbols and metaphors used\n",
    "\n",
    "    Format your response as JSON with these keys: moral, characters, structure, symbols\"\"\"\n",
    "    \n",
    "    user_prompt = f\"Analyze this fable:\\n\\n{original_fable}\"\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ])\n",
    "    \n",
    "    # Track the LLM call\n",
    "    track_llm_call(\n",
    "        node_name=\"analyze_fable\",\n",
    "        tool_name=\"aesop_tool\",\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        response_text=response.content\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        analysis = json.loads(response.content)\n",
    "    except:\n",
    "        # Fallback if JSON parsing fails\n",
    "        print(\"JSON parsing failed, using content as analysis\")\n",
    "        analysis = {\n",
    "            \"moral\": response.content,\n",
    "            \"characters\": [],\n",
    "            \"structure\": {},\n",
    "            \"symbols\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Analysis complete: identified moral '{analysis.get('moral', '')[:50]}...'\")\n",
    "    return {\"analysis\": analysis}\n",
    "\n",
    "@track_node(\"brainstorm_story\", \"aesop_tool\")\n",
    "# Node 2: Brainstorm Story\n",
    "def brainstorm_story(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Brainstorms ideas for story creation based on analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Brainstorm Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    \n",
    "    print(f\"Brainstorming based on analysis of moral: {analysis.get('moral', '')[:50]}...\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=BRAINSTORM_STORY_PROMPT),\n",
    "        HumanMessage(content=f\"Original fable: {original_fable}\\n\\nAnalysis: {json.dumps(analysis, indent=2)}\")\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        brainstorm = json.loads(response.content)\n",
    "    except:\n",
    "        print(\"JSON parsing failed, using content as brainstorm\")\n",
    "        brainstorm = {\n",
    "            \"moral_approaches\": response.content,\n",
    "            \"variations\": [],\n",
    "            \"character_ideas\": [],\n",
    "            \"imagery\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Brainstorming complete: generated {len(brainstorm.keys())} idea categories\")\n",
    "    return {\"brainstorm\": brainstorm}\n",
    "\n",
    "\n",
    "@track_node(\"generate_story\", \"aesop_tool\")\n",
    "# Node 3: Generate Story\n",
    "def generate_story(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates the final story based on analysis and brainstorming\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Generate Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    brainstorm = state.get(\"brainstorm\", {})\n",
    "    \n",
    "    print(f\"Generating story based on analysis and brainstorming\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=GENERATE_STORY_PROMPT),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Original fable: {original_fable}\n",
    "        \n",
    "        Analysis: {json.dumps(analysis, indent=2)}\n",
    "        \n",
    "        Brainstorming: {json.dumps(brainstorm, indent=2)}\n",
    "        \n",
    "        Create a refined version of this fable.\n",
    "        \"\"\")\n",
    "    ])\n",
    "    \n",
    "    generated_story = response.content\n",
    "    print(f\"Story generated (length: {len(generated_story)} characters)\")\n",
    "    \n",
    "    return {\"generated_story\": generated_story}\n",
    "\n",
    "# Cell 8: BUILD THE AESOP SUBGRAPH\n",
    "def build_aesop_subgraph():\n",
    "    \"\"\"\n",
    "    Builds the Aesop tool subgraph\n",
    "    \"\"\"\n",
    "    builder = StateGraph(AesopState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"analyze_fable\", analyze_fable)\n",
    "    builder.add_node(\"brainstorm_story\", brainstorm_story)\n",
    "    builder.add_node(\"generate_story\", generate_story)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"analyze_fable\")\n",
    "    builder.add_edge(\"analyze_fable\", \"brainstorm_story\")\n",
    "    builder.add_edge(\"brainstorm_story\", \"generate_story\")\n",
    "    builder.add_edge(\"generate_story\", END)\n",
    "    \n",
    "    # Compile\n",
    "    return builder.compile()\n",
    "\n",
    "# Cell 9: AESOP SUBGRAPH WRAPPER\n",
    "def aesop_subgraph(processing_request: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Wrapper function that runs the Aesop subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Running Aesop Subgraph ===\")\n",
    "    \n",
    "    # Create initial state for Aesop subgraph\n",
    "    initial_state = {\n",
    "        \"original_fable\": processing_request.get(\"fable_text\", \"\"),\n",
    "        \"analysis\": {},\n",
    "        \"brainstorm\": {},\n",
    "        \"generated_story\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Build and run the subgraph\n",
    "    aesop_graph = build_aesop_subgraph()\n",
    "    result = aesop_graph.invoke(initial_state)\n",
    "    \n",
    "    # Return the enriched state\n",
    "    return {\n",
    "        \"analysis\": result[\"analysis\"],\n",
    "        \"brainstorm\": result[\"brainstorm\"],\n",
    "        \"generated_story\": result[\"generated_story\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e33a4e",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "Build the main graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4aea35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: BUILD THE MAIN GRAPH\n",
    "def build_main_graph():\n",
    "    \"\"\"\n",
    "    Builds the main orchestrator graph\n",
    "    \"\"\"\n",
    "    builder = StateGraph(MainState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"main_agent\", main_agent)\n",
    "    builder.add_node(\"tool_router\", tool_router)\n",
    "    builder.add_node(\"generate_output\", generate_output)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"main_agent\")\n",
    "    builder.add_edge(\"main_agent\", \"tool_router\")\n",
    "    builder.add_edge(\"tool_router\", \"generate_output\")\n",
    "    builder.add_edge(\"generate_output\", END)\n",
    "    \n",
    "    # Compile\n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c676ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: TEST SYSTEM\n",
    "def test_system():\n",
    "    # Build the main graph\n",
    "    main_graph = build_main_graph()\n",
    "    print(\"Graph built successfully!\")\n",
    "    \n",
    "    # Test with a sample Aesop fable\n",
    "    test_fable = \"\"\"\n",
    "    The Fox and the Grapes\n",
    "    \n",
    "    A hungry Fox saw some fine bunches of Grapes hanging from a vine that was trained along a high trellis, and did his best to reach them by jumping as high as he could into the air. But it was all in vain, for they were just out of reach: so he gave up trying, and walked away with an air of dignity and unconcern, remarking, \"I thought those Grapes were ripe, but I see now they are quite sour.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze and enhance this fable\"}],\n",
    "        \"current_fable\": test_fable,\n",
    "        \"tool_to_call\": \"\",\n",
    "        \"processing_request\": {},\n",
    "        \"tool_output\": {},\n",
    "        \"final_story\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    result = main_graph.invoke(initial_state)\n",
    "    \n",
    "    # Finish tracking this story\n",
    "    story_stats = finish_story(result[\"final_story\"])\n",
    "    \n",
    "    print(\"\\n=== FINAL RESULT ===\")\n",
    "    print(result[\"final_story\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70848946",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e07bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b99aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece30471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47376e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storyforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
