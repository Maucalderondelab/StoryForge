{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc8937f",
   "metadata": {},
   "source": [
    "# Generate the agent\n",
    "This notebook latter we will pass it into the `base agents.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d46791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Typing\n",
    "from typing import Dict, List, Any, Optional, TypedDict\n",
    "from IPython.display import Image, display\n",
    "\n",
    "#LangGraph/LangChain\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI  # or any other LLM provider\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# Prompts\n",
    "#Load env files\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72d2e",
   "metadata": {},
   "source": [
    "## Systems Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Agent Prompts (largely unchanged since it's the orchestrator)\n",
    "MAIN_AGENT_PROMPT = \"\"\"You are an AI orchestrator for a story processing system. \n",
    "Analyze the user's request and the provided text to determine:\n",
    "1. Is this an Aesop's fable? (yes/no)\n",
    "2. What action should be taken? (analyze/retell/expand/modernize/create_new)\n",
    "3. Any specific requirements? (style, moral, characters)\n",
    "\n",
    "Respond in JSON format with keys: is_aesop, action, requirements\"\"\"\n",
    "\n",
    "# Aesop Tool Prompts - Enhanced Analysis\n",
    "ANALYZE_FABLE_PROMPT = \"\"\"You are an expert in Aesop's fables and modern storytelling. Analyze the given fable and provide:\n",
    "1. The core moral/lesson (what universal truth is it teaching?)\n",
    "2. The conflict pattern (what fundamental challenge/dilemma does it present?)\n",
    "3. Character archetypes and their essential traits (what roles do they serve?)\n",
    "4. The narrative structure (setup, challenge, resolution)\n",
    "5. What makes this moral relevant today\n",
    "\n",
    "Format your response as JSON with these keys: moral, conflict_pattern, characters, structure, modern_relevance\"\"\"\n",
    "\n",
    "# Enhanced Brainstorming for Modern Micro-Fables\n",
    "BRAINSTORM_STORY_PROMPT = \"\"\"You are a creative storyteller specializing in ultra-short modern fables for social media.\n",
    "Based on the analysis provided, brainstorm ideas for a 100-110 word modern fable:\n",
    "\n",
    "1. Fresh animal substitutions: Replace the original animals with unexpected species that maintain the same character traits but feel more surprising (consider unusual animals from diverse ecosystems)\n",
    "\n",
    "2. Innovative settings: Suggest 2-3 unique settings beyond traditional forests (coral reefs, urban environments, cosmic settings, etc.)\n",
    "\n",
    "3. Modern context: How could the core conflict be reframed in a contemporary or unexpected context while preserving the moral?\n",
    "\n",
    "4. Implicit teaching approach: How to convey the moral without explicitly stating it, followed by a memorable 5-10 word takeaway phrase\n",
    "\n",
    "Format your response as JSON with these keys: animal_substitutions, settings, modern_context, implicit_teaching, takeaway_phrase\"\"\"\n",
    "\n",
    "# Enhanced Story Generation for TikTok-Style Fables\n",
    "GENERATE_STORY_AESOP_PROMPT = \"\"\"You are a master of micro-storytelling creating modern Aesop fables for the TikTok generation.\n",
    "Create a compelling 120-130 word fable that:\n",
    "\n",
    "1. Uses the suggested animal substitutions and innovative setting\n",
    "2. Presents a complete narrative arc (setup, conflict, resolution) with extreme efficiency\n",
    "3. Teaches the moral implicitly through the story\n",
    "4. Uses vivid, sensory language that creates mental images\n",
    "5. Ends with the suggested 5-10 word takeaway phrase (not an explicit \"the moral is...\")\n",
    "\n",
    "STRICT CONSTRAINTS:\n",
    "- Exactly 110-100 words for the main story\n",
    "- Takeaway phrase should be 5-10 words, positioned at the end\n",
    "- No explicit statement of \"the moral is...\" or \"this teaches us...\" in the story\n",
    "- Every word must serve multiple purposes (character, plot, and theme)\n",
    "\n",
    "Your task is to distill ancient wisdom into a shareable modern micro-fable.\"\"\"\n",
    "\n",
    "# Enhanced Output Formatting\n",
    "FORMAT_OUTPUT_PROMPT = \"\"\"Format this modern Aesop fable for maximum impact on social platforms:\n",
    "\n",
    "1. Present the story as a complete micro-fable (exactly as generated, preserving the 100-110 word count)\n",
    "2. Set the takeaway phrase on its own line at the end, styled for emphasis\n",
    "3. Include a brief note about the original fable it's based on\n",
    "4. Mention one interesting insight about how this modern version preserves the timeless wisdom\n",
    "\n",
    "Keep the entire output concise and visually scannable - perfect for a quick digital read.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440513b",
   "metadata": {},
   "source": [
    "## Create metadata class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d10906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import functools\n",
    "\n",
    "from datetime import datetime\n",
    "import tiktoken  # For token counting\n",
    "\n",
    "# Global metadata store\n",
    "metadata = {\n",
    "    \"session_start\": time.time(),\n",
    "    \"session_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"current_story\": None,\n",
    "    \"stories\": {},\n",
    "    \"total_tokens\": 0,\n",
    "    \"total_cost\": 0,\n",
    "    \"total_time\": 0\n",
    "}\n",
    "\n",
    "def track_node(node_name, tool_name=\"main\"):\n",
    "    \"\"\"Decorator to track execution time of nodes\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(state):\n",
    "            # Get current story ID or create one\n",
    "            story_id = metadata.get(\"current_story\")\n",
    "            if not story_id:\n",
    "                story_id = f\"story_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                metadata[\"current_story\"] = story_id\n",
    "                metadata[\"stories\"][story_id] = {\n",
    "                    \"start_time\": time.time(),\n",
    "                    \"nodes\": {},\n",
    "                    \"tools\": {},\n",
    "                    \"llm_calls\": [],\n",
    "                    \"total_tokens\": 0,\n",
    "                    \"total_cost\": 0\n",
    "                }\n",
    "            \n",
    "            # Initialize node data if needed\n",
    "            story = metadata[\"stories\"][story_id]\n",
    "            if node_name not in story[\"nodes\"]:\n",
    "                story[\"nodes\"][node_name] = {\n",
    "                    \"calls\": 0,\n",
    "                    \"total_time\": 0,\n",
    "                    \"tokens\": 0\n",
    "                }\n",
    "            \n",
    "            # Start timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Call the function\n",
    "            try:\n",
    "                result = func(state)\n",
    "                # Record timing data\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                # Update metrics\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                \n",
    "                print(f\"Node {node_name} executed in {duration:.2f} seconds\")\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                # Record error but re-raise\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                print(f\"Error in {node_name}: {str(e)}\")\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                story[\"nodes\"][node_name][\"errors\"] = story[\"nodes\"][node_name].get(\"errors\", 0) + 1\n",
    "                \n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def track_llm_call(node_name, tool_name, model, system_prompt, user_prompt, response_text):\n",
    "    \"\"\"Track an LLM API call\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    # Simple token estimation (very rough)\n",
    "    input_tokens = (len(system_prompt) + len(user_prompt)) // 4  # ~4 chars per token\n",
    "    output_tokens = len(response_text) // 4\n",
    "    \n",
    "    # Cost estimation (very rough)\n",
    "    if model == \"gpt-4.1-mini\":\n",
    "        input_cost = (input_tokens / 1000) * 0.00015\n",
    "        output_cost = (output_tokens / 1000) * 0.00060\n",
    "    else:\n",
    "        input_cost = (input_tokens / 1000) * 0.00010\n",
    "        output_cost = (output_tokens / 1000) * 0.00030\n",
    "    \n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    # Record the call\n",
    "    call_data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"node\": node_name,\n",
    "        \"tool\": tool_name,\n",
    "        \"model\": model,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": input_tokens + output_tokens,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "    \n",
    "    # Update story\n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"llm_calls\"].append(call_data)\n",
    "    story[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    story[\"total_cost\"] += total_cost\n",
    "    \n",
    "    # Update node\n",
    "    if node_name in story[\"nodes\"]:\n",
    "        story[\"nodes\"][node_name][\"tokens\"] += input_tokens + output_tokens\n",
    "    \n",
    "    # Update global\n",
    "    metadata[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    metadata[\"total_cost\"] += total_cost\n",
    "    \n",
    "    print(f\"LLM call in {node_name}: {input_tokens + output_tokens} tokens, ${total_cost:.4f}\")\n",
    "\n",
    "def finish_story(output_text):\n",
    "    \"\"\"Finish tracking the current story\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"end_time\"] = time.time()\n",
    "    story[\"duration\"] = story[\"end_time\"] - story[\"start_time\"]\n",
    "    story[\"output_length\"] = len(output_text)\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    total_time = sum(node[\"total_time\"] for node in story[\"nodes\"].values())\n",
    "    total_calls = sum(node[\"calls\"] for node in story[\"nodes\"].values())\n",
    "    \n",
    "    print(\"\\n=== STORY METRICS ===\")\n",
    "    print(f\"Total execution time: {story['duration']:.2f} seconds\")\n",
    "    print(f\"Total tokens: {story['total_tokens']} tokens\")\n",
    "    print(f\"Estimated cost: ${story['total_cost']:.4f}\")\n",
    "    print(f\"Number of nodes executed: {len(story['nodes'])}\")\n",
    "    print(f\"Number of LLM calls: {len(story['llm_calls'])}\")\n",
    "    \n",
    "    # Reset current story\n",
    "    metadata[\"current_story\"] = None\n",
    "    metadata[\"total_time\"] += story[\"duration\"]\n",
    "    \n",
    "    # Export metadata\n",
    "    with open(f\"story_metrics_{story_id}.json\", \"w\") as f:\n",
    "        json.dump(story, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Metrics saved to story_metrics_{story_id}.json\")\n",
    "    \n",
    "    return story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197d69b",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Import OPENAI as a global variable and define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "373f8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## Call the api keys from the .env file\n",
    "llm_openai_41_mini = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899b965",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Define the the states of the main graph and the subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a7c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainState(TypedDict):\n",
    "    messages: List[Dict[str, Any]]  # User messages\n",
    "    current_fable: str              # Input fable text\n",
    "    tool_to_call: str               # Which tool subgraph to use\n",
    "    processing_request: Dict        # Request info for the tool\n",
    "    tool_output: Dict               # Output from selected tool\n",
    "    final_story: str                # Final output after post-processing\n",
    "\n",
    "# Cell 3: Define Aesop State (for subgraph)\n",
    "class AesopState(TypedDict):\n",
    "    original_fable: str             # Original input text\n",
    "    analysis: Dict                  # Analysis of the fable (moral, characters, etc)\n",
    "    brainstorm: Dict                # Ideas for the story creation/modification\n",
    "    generated_story: str            # The final story created by this tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f881a4a",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "Create the story router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ddea820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Main Agent for the main graph\n",
    "@track_node(\"main_agent\", \"main\")\n",
    "def main_agent(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main orchestrator that analyzes input and decides which tool to use\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Main Agent ===\")\n",
    "    current_fable = state.get(\"current_fable\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    user_message = messages[-1].get(\"content\", \"\") if messages else \"\"\n",
    "    print(f\"Processing request: {user_message}\")\n",
    "    print(f\"Current fable length: {len(current_fable)} characters\")\n",
    "    \n",
    "    is_aesop = True\n",
    "    \n",
    "    if is_aesop:\n",
    "        processing_request = {\n",
    "            \"user_intent\": user_message,\n",
    "            \"fable_text\": current_fable\n",
    "        }\n",
    "        \n",
    "        result = {\n",
    "            \"processing_request\": processing_request,\n",
    "            \"tool_to_call\": \"aesop_tool\"\n",
    "        }\n",
    "        print(f\"Selecting tool: aesop_tool\")\n",
    "        return result\n",
    "    else:\n",
    "        # Future expansion for other tools\n",
    "        return {\n",
    "            \"processing_request\": {},\n",
    "            \"tool_to_call\": \"generic_tool\" \n",
    "        }\n",
    "\n",
    "# Cell 5: Tool Router\n",
    "def tool_router(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Routes to the appropriate tool subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Tool Router ===\")\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    processing_request = state.get(\"processing_request\", {})\n",
    "    \n",
    "    print(f\"Routing to tool: {tool_name}\")\n",
    "    \n",
    "    if tool_name == \"aesop_tool\":\n",
    "        # Call the Aesop subgraph\n",
    "        aesop_result = aesop_subgraph(processing_request)\n",
    "        print(f\"Received result from Aesop tool\")\n",
    "        return {\"tool_output\": aesop_result}\n",
    "    else:\n",
    "        # Future: add more tool subgraphs\n",
    "        return {\"tool_output\": {\"error\": \"Tool not found\"}}\n",
    "\n",
    "@track_node(\"generate_output\", \"main\")\n",
    "def generate_output(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the final output based on the tool results\n",
    "    Creates a two-part story with strict word count limits\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generate Output ===\")\n",
    "    tool_output = state.get(\"tool_output\", {})\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    \n",
    "    if \"error\" in tool_output:\n",
    "        final_story = f\"Error: {tool_output['error']}\"\n",
    "    else:\n",
    "        if tool_name == \"aesop_tool\":\n",
    "            # Get story components from the tool output\n",
    "            generated_story = tool_output.get(\"generated_story\", \"\")\n",
    "            analysis = tool_output.get(\"analysis\", {})\n",
    "            brainstorm = tool_output.get(\"brainstorm\", {})\n",
    "            \n",
    "            # Track the original story length for metadata\n",
    "            original_word_count = len(generated_story.split())\n",
    "            print(f\"Original story word count: {original_word_count}\")\n",
    "            \n",
    "            # Create a prompt to split and enhance the story with strict word counts\n",
    "            system_prompt = \"\"\"You are a master storyteller specializing in micro-fables for social media.\n",
    "            Take this fable and transform it into a two-part story with STRICT word count limits:\n",
    "            \n",
    "            PART 1 (First Post):\n",
    "            - EXACTLY 80-90 WORDS MAXIMUM\n",
    "            - Contains the setup, characters, and builds tension\n",
    "            - Ends at a compelling moment that makes readers curious for the conclusion\n",
    "            - Should be clearly labeled as \"PART 1\"\n",
    "            \n",
    "            PART 2 (Second Post):\n",
    "            - EXACTLY 80-90 WORDS MAXIMUM (including the recap)\n",
    "            - Begins with a brief 1-sentence recap of Part 1, and explicitly add at the begginig In Part 1\n",
    "            - Contains the resolution and moral/takeaway\n",
    "            - Ends with a memorable phrase that captures the moral\n",
    "            - Should be clearly labeled as \"PART 2\"\n",
    "            \n",
    "            The word counts are STRICT REQUIREMENTS - do not exceed 80 words for each part.\n",
    "            Count your words carefully before finalizing each part.\n",
    "            \"\"\"\n",
    "            \n",
    "            user_prompt = f\"\"\"Story: {generated_story}\n",
    "            \n",
    "            Analysis: {json.dumps(analysis, indent=2)}\n",
    "            \n",
    "            Split this into a two-part story as described, with exactly 80 words maximum for each part.\n",
    "            Ensure Part 2 begins with a brief recap so viewers can understand the conclusion even if they missed Part 1.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Call LLM to reformat the story\n",
    "            response = llm_openai_41_mini.invoke([\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=user_prompt)\n",
    "            ])\n",
    "            \n",
    "            # Track LLM call\n",
    "            track_llm_call(\n",
    "                node_name=\"generate_output\",\n",
    "                tool_name=\"main\",\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                response_text=response.content\n",
    "            )\n",
    "            \n",
    "            # The final reformatted story\n",
    "            final_story = response.content\n",
    "            \n",
    "            # Track final word count for metadata\n",
    "            final_word_count = len(final_story.split())\n",
    "            print(f\"Final story word count: {final_word_count}\")\n",
    "            \n",
    "            # Verify the word counts of each part (for validation and debugging)\n",
    "            parts = final_story.split(\"PART 2\")\n",
    "            if len(parts) > 1:\n",
    "                part1 = parts[0].replace(\"PART 1\", \"\").strip()\n",
    "                part2 = \"PART 2\" + parts[1].strip()\n",
    "                \n",
    "                part1_words = len(part1.split())\n",
    "                part2_words = len(part2.split())\n",
    "                \n",
    "                print(f\"Part 1 word count: {part1_words}\")\n",
    "                print(f\"Part 2 word count: {part2_words}\")\n",
    "                \n",
    "                # Add to metadata\n",
    "                if \"current_story\" in metadata and metadata[\"current_story\"] in metadata[\"stories\"]:\n",
    "                    story_id = metadata[\"current_story\"]\n",
    "                    if \"transformations\" not in metadata[\"stories\"][story_id]:\n",
    "                        metadata[\"stories\"][story_id][\"transformations\"] = {}\n",
    "                    \n",
    "                    metadata[\"stories\"][story_id][\"transformations\"][\"story_split\"] = {\n",
    "                        \"original_word_count\": original_word_count,\n",
    "                        \"final_word_count\": final_word_count,\n",
    "                        \"part1_word_count\": part1_words,\n",
    "                        \"part2_word_count\": part2_words,\n",
    "                        \"transformation_type\": \"two-part-fixed-length\",\n",
    "                        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }\n",
    "                    \n",
    "                    print(\"Added transformation metadata\")\n",
    "        else:\n",
    "            # Future: handle other tool outputs with different formatting strategies\n",
    "            final_story = str(tool_output)\n",
    "    \n",
    "    print(f\"Final story generated (length: {len(final_story)} characters)\")\n",
    "    return {\"final_story\": final_story}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e6013",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "Create the tools-subgraphs, currently we have:\n",
    "* Aesop tool-subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8b4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Analyze Fable with tracking\n",
    "@track_node(\"analyze_fable\", \"aesop_tool\")\n",
    "def analyze_fable(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes the fable structure, characters, and moral\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Analyze Fable ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    print(f\"Analyzing fable (length: {len(original_fable)} characters)\")\n",
    "    \n",
    "    # Use LLM to analyze the fable with prompt from prompts.py\n",
    "    system_prompt = ANALYZE_FABLE_PROMPT\n",
    "    \n",
    "    user_prompt = f\"Analyze this fable:\\n\\n{original_fable}\"\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ])\n",
    "    \n",
    "    # Track the LLM call\n",
    "    track_llm_call(\n",
    "        node_name=\"analyze_fable\",\n",
    "        tool_name=\"aesop_tool\",\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        response_text=response.content\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        analysis = json.loads(response.content)\n",
    "    except:\n",
    "        # Fallback if JSON parsing fails\n",
    "        print(\"JSON parsing failed, using content as analysis\")\n",
    "        analysis = {\n",
    "            \"moral\": response.content,\n",
    "            \"characters\": [],\n",
    "            \"structure\": {},\n",
    "            \"symbols\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Analysis complete: identified moral '{analysis.get('moral', '')[:50]}...'\")\n",
    "    return {\"analysis\": analysis}\n",
    "\n",
    "@track_node(\"brainstorm_story\", \"aesop_tool\")\n",
    "# Node 2: Brainstorm Story\n",
    "def brainstorm_story(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Brainstorms ideas for story creation based on analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Brainstorm Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    \n",
    "    print(f\"Brainstorming based on analysis of moral: {analysis.get('moral', '')[:50]}...\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=BRAINSTORM_STORY_PROMPT),\n",
    "        HumanMessage(content=f\"Original fable: {original_fable}\\n\\nAnalysis: {json.dumps(analysis, indent=2)}\")\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        brainstorm = json.loads(response.content)\n",
    "    except:\n",
    "        print(\"JSON parsing failed, using content as brainstorm\")\n",
    "        brainstorm = {\n",
    "            \"moral_approaches\": response.content,\n",
    "            \"variations\": [],\n",
    "            \"character_ideas\": [],\n",
    "            \"imagery\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Brainstorming complete: generated {len(brainstorm.keys())} idea categories\")\n",
    "    return {\"brainstorm\": brainstorm}\n",
    "\n",
    "\n",
    "@track_node(\"generate_story_aessop\", \"aesop_tool\")\n",
    "# Node 3: Generate Story\n",
    "def generate_story_aessop(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates the final story based on analysis and brainstorming\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Generate Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    brainstorm = state.get(\"brainstorm\", {})\n",
    "    \n",
    "    print(f\"Generating story based on analysis and brainstorming\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=GENERATE_STORY_AESOP_PROMPT),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Original fable: {original_fable}\n",
    "        \n",
    "        Analysis: {json.dumps(analysis, indent=2)}\n",
    "        \n",
    "        Brainstorming: {json.dumps(brainstorm, indent=2)}\n",
    "        \n",
    "        Create a refined version of this fable.\n",
    "        \"\"\")\n",
    "    ])\n",
    "    \n",
    "    generated_story = response.content\n",
    "    print(f\"Story generated (length: {len(generated_story)} characters)\")\n",
    "    \n",
    "    return {\"generated_story\": generated_story}\n",
    "\n",
    "# Cell 8: BUILD THE AESOP SUBGRAPH\n",
    "def build_aesop_subgraph():\n",
    "    \"\"\"\n",
    "    Builds the Aesop tool subgraph\n",
    "    \"\"\"\n",
    "    builder = StateGraph(AesopState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"analyze_fable\", analyze_fable)\n",
    "    builder.add_node(\"brainstorm_story\", brainstorm_story)\n",
    "    builder.add_node(\"generate_story_aessop\", generate_story_aessop)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"analyze_fable\")\n",
    "    builder.add_edge(\"analyze_fable\", \"brainstorm_story\")\n",
    "    builder.add_edge(\"brainstorm_story\", \"generate_story_aessop\")\n",
    "    builder.add_edge(\"generate_story_aessop\", END)\n",
    "    \n",
    "    # Compile\n",
    "    return builder.compile()\n",
    "\n",
    "# Cell 9: AESOP SUBGRAPH WRAPPER\n",
    "def aesop_subgraph(processing_request: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Wrapper function that runs the Aesop subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Running Aesop Subgraph ===\")\n",
    "    \n",
    "    # Create initial state for Aesop subgraph\n",
    "    initial_state = {\n",
    "        \"original_fable\": processing_request.get(\"fable_text\", \"\"),\n",
    "        \"analysis\": {},\n",
    "        \"brainstorm\": {},\n",
    "        \"generated_story\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Build and run the subgraph\n",
    "    aesop_graph = build_aesop_subgraph()\n",
    "    result = aesop_graph.invoke(initial_state)\n",
    "    \n",
    "    # Return the enriched state\n",
    "    return {\n",
    "        \"analysis\": result[\"analysis\"],\n",
    "        \"brainstorm\": result[\"brainstorm\"],\n",
    "        \"generated_story\": result[\"generated_story\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e33a4e",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "Build the main graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aea35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: BUILD THE MAIN GRAPH\n",
    "def build_main_graph():\n",
    "    \"\"\"\n",
    "    Builds the main orchestrator graph\n",
    "    \"\"\"\n",
    "    builder = StateGraph(MainState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"main_agent\", main_agent)\n",
    "    builder.add_node(\"tool_router\", tool_router)\n",
    "    builder.add_node(\"generate_output\", generate_output)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"main_agent\")\n",
    "    builder.add_edge(\"main_agent\", \"tool_router\")\n",
    "    builder.add_edge(\"tool_router\", \"generate_output\")\n",
    "    builder.add_edge(\"generate_output\", END)\n",
    "    \n",
    "    # Compile\n",
    "    return builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c676ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: TEST SYSTEM\n",
    "def test_system():\n",
    "    # Build the main graph\n",
    "    main_graph = build_main_graph()\n",
    "    print(\"Graph built successfully!\")\n",
    "    \n",
    "    # Test with a sample Aesop fable\n",
    "    test_fable = \"\"\"\n",
    "    The Fox and the Grapes\n",
    "    \n",
    "    A hungry Fox saw some fine bunches of Grapes hanging from a vine that was trained along a high trellis, and did his best to reach them by jumping as high as he could into the air. But it was all in vain, for they were just out of reach: so he gave up trying, and walked away with an air of dignity and unconcern, remarking, \"I thought those Grapes were ripe, but I see now they are quite sour.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze and enhance this fable\"}],\n",
    "        \"current_fable\": test_fable,\n",
    "        \"tool_to_call\": \"\",\n",
    "        \"processing_request\": {},\n",
    "        \"tool_output\": {},\n",
    "        \"final_story\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    result = main_graph.invoke(initial_state)\n",
    "    \n",
    "    # Finish tracking this story\n",
    "    story_stats = finish_story(result[\"final_story\"])\n",
    "    \n",
    "    print(\"\\n=== FINAL RESULT ===\")\n",
    "    print(result[\"final_story\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70848946",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "436e07bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built successfully!\n",
      "\n",
      "=== Main Agent ===\n",
      "Processing request: Analyze and enhance this fable\n",
      "Current fable length: 432 characters\n",
      "Selecting tool: aesop_tool\n",
      "Node main_agent executed in 0.00 seconds\n",
      "\n",
      "=== Tool Router ===\n",
      "Routing to tool: aesop_tool\n",
      "\n",
      "=== Running Aesop Subgraph ===\n",
      "\n",
      "== Aesop Subgraph: Analyze Fable ==\n",
      "Analyzing fable (length: 432 characters)\n",
      "LLM call in analyze_fable: 604 tokens, $0.0003\n",
      "Analysis complete: identified moral 'It is easy to despise what you cannot have; people...'\n",
      "Node analyze_fable executed in 4.45 seconds\n",
      "\n",
      "== Aesop Subgraph: Brainstorm Story ==\n",
      "Brainstorming based on analysis of moral: It is easy to despise what you cannot have; people...\n",
      "Brainstorming complete: generated 5 idea categories\n",
      "Node brainstorm_story executed in 3.38 seconds\n",
      "\n",
      "== Aesop Subgraph: Generate Story ==\n",
      "Generating story based on analysis and brainstorming\n",
      "Story generated (length: 710 characters)\n",
      "Node generate_story executed in 2.66 seconds\n",
      "Received result from Aesop tool\n",
      "\n",
      "=== Generate Output ===\n",
      "Original story word count: 106\n",
      "LLM call in generate_output: 1093 tokens, $0.0003\n",
      "Final story word count: 146\n",
      "Part 1 word count: 77\n",
      "Part 2 word count: 66\n",
      "Added transformation metadata\n",
      "Final story generated (length: 926 characters)\n",
      "Node generate_output executed in 2.93 seconds\n",
      "\n",
      "=== STORY METRICS ===\n",
      "Total execution time: 13.42 seconds\n",
      "Total tokens: 1697 tokens\n",
      "Estimated cost: $0.0005\n",
      "Number of nodes executed: 5\n",
      "Number of LLM calls: 2\n",
      "Metrics saved to story_metrics_story_20250518_005337.json\n",
      "\n",
      "=== FINAL RESULT ===\n",
      "PART 1  \n",
      "Beneath neon reefs, a curious octopus spotted glowing algae atop towering spires. Its eight arms stretched, curling, weaving clever traps to snatch the luminous feast. Pulses of light flickered just beyond reach—taunting, teasing. Breath tightening, it dove higher, grasping but slipping, yearning. Despite its efforts, the algae clung steadfast, untouched. Defeat weighed heavy as its skin dimmed to dull grey. Was the prize truly out of reach, or was there a deeper secret lurking beneath the glow?  \n",
      "\n",
      "PART 2  \n",
      "In Part 1, the octopus struggled to grab glowing algae beyond its reach. Defeated, it whispered to cuttlefish, “Those lights must be laced with poison.” Around them, coral hummed, hinting at secrets: some prizes shine too bright to claim, and some truths are veiled by bitter tales. Sometimes, we dismiss what we cannot have, disguising loss with disdain. Remember—sour grapes often hide a sweeter truth.\n"
     ]
    }
   ],
   "source": [
    "# Cell to actually run the test\n",
    "test_result = test_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2b99aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece30471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47376e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storyforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
