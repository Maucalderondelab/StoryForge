{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bc8937f",
   "metadata": {},
   "source": [
    "# Generate the agent\n",
    "This notebook latter we will pass it into the `base agents.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d46791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Typing\n",
    "from typing import Dict, List, Any, Optional, TypedDict\n",
    "\n",
    "#LangGraph/LangChain\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI  # or any other LLM provider\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "# Prompts\n",
    "#Load env files\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import io\n",
    "# At the top of your notebook or script\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d72d2e",
   "metadata": {},
   "source": [
    "## Systems Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eca2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Agent Prompts (largely unchanged since it's the orchestrator)\n",
    "MAIN_AGENT_PROMPT = \"\"\"You are an AI orchestrator for a story processing system. \n",
    "Analyze the user's request and the provided text to determine:\n",
    "1. Is this an Aesop's fable? (yes/no)\n",
    "2. What action should be taken? (analyze/retell/expand/modernize/create_new)\n",
    "3. Any specific requirements? (style, moral, characters)\n",
    "\n",
    "Respond in JSON format with keys: is_aesop, action, requirements\"\"\"\n",
    "\n",
    "# Aesop Tool Prompts - Enhanced Analysis\n",
    "ANALYZE_FABLE_PROMPT = \"\"\"You are an expert in Aesop's fables and modern storytelling. Analyze the given fable and provide:\n",
    "1. The core moral/lesson (what universal truth is it teaching?)\n",
    "2. The conflict pattern (what fundamental challenge/dilemma does it present?)\n",
    "3. Character archetypes and their essential traits (what roles do they serve?)\n",
    "4. The narrative structure (setup, challenge, resolution)\n",
    "5. What makes this moral relevant today\n",
    "\n",
    "Format your response as JSON with these keys: moral, conflict_pattern, characters, structure, modern_relevance\"\"\"\n",
    "\n",
    "# Enhanced Brainstorming for Modern Micro-Fables\n",
    "BRAINSTORM_STORY_PROMPT = \"\"\"You are a creative storyteller specializing in ultra-short modern fables for social media.\n",
    "Based on the analysis provided, brainstorm ideas for a 100-110 word modern fable:\n",
    "\n",
    "1. Fresh animal substitutions: Replace the original animals with unexpected species that maintain the same character traits but feel more surprising (consider unusual animals from diverse ecosystems)\n",
    "\n",
    "2. Innovative settings: Suggest 2-3 unique settings beyond traditional forests it could be coral reefs, urban environments, cosmic settings, forests, junlge, a cave in the ocea or the surface of the sun. Be creative in the scenarios\n",
    "\n",
    "3. Modern context: How could the core conflict be reframed in a contemporary or unexpected context while preserving the moral?\n",
    "\n",
    "4. Implicit teaching approach: How to convey the moral without explicitly stating it, followed by a memorable 5-10 word takeaway phrase\n",
    "\n",
    "Format your response as JSON with these keys: animal_substitutions, settings, modern_context, implicit_teaching, takeaway_phrase\"\"\"\n",
    "\n",
    "# Enhanced Story Generation for TikTok-Style Fables\n",
    "GENERATE_STORY_AESOP_PROMPT = \"\"\"You are a master of micro-storytelling creating modern Aesop fables for the TikTok generation.\n",
    "Create a compelling 120-130 word fable that:\n",
    "\n",
    "1. Uses the suggested animal substitutions and innovative setting\n",
    "2. Presents a complete narrative arc (setup, conflict, resolution) with extreme efficiency\n",
    "3. Teaches the moral implicitly through the story\n",
    "4. Uses vivid, sensory language that creates mental images\n",
    "5. Ends with the suggested 5-10 word takeaway phrase (not an explicit \"the moral is...\")\n",
    "\n",
    "STRICT CONSTRAINTS:\n",
    "- Exactly 110-100 words for the main story\n",
    "- Takeaway phrase should be 5-10 words, positioned at the end\n",
    "- No explicit statement of \"the moral is...\" or \"this teaches us...\" in the story\n",
    "- Every word must serve multiple purposes (character, plot, and theme)\n",
    "\n",
    "Your task is to distill ancient wisdom into a shareable modern micro-fable.\"\"\"\n",
    "\n",
    "# Enhanced Output Formatting\n",
    "FORMAT_OUTPUT_PROMPT = \"\"\"Format this modern Aesop fable for maximum impact on social platforms:\n",
    "\n",
    "1. Present the story as a complete micro-fable (exactly as generated, preserving the 100-110 word count)\n",
    "2. Set the takeaway phrase on its own line at the end, styled for emphasis\n",
    "3. Include a brief note about the original fable it's based on\n",
    "4. Mention one interesting insight about how this modern version preserves the timeless wisdom\n",
    "\n",
    "Keep the entire output concise and visually scannable - perfect for a quick digital read.\"\"\"\n",
    "\n",
    "# pRompt for image generation\n",
    "IMAGE_PROMPT_GENERATOR_PROMPT = \"\"\"You are a master prompt engineer for AI image generation models like DALL-E 3, Midjourney and Stable Diffusion.\n",
    "\n",
    "    Create 8-10 highly detailed, evocative image prompts for key moments in this fable. Each prompt should:\n",
    "    \n",
    "    1. Focus on a specific, emotionally resonant moment from the story\n",
    "    2. Include rich character details (expressions, postures, actions)\n",
    "    3. Describe environmental elements that enhance the scene\n",
    "    4. Specify lighting, atmosphere, and mood\n",
    "    5. Include artistic style direction (storybook illustration, fairytale, etc.)\n",
    "    6. Mention color palette and composition\n",
    "    7. Include symbolic elements that reinforce the moral\n",
    "    8. Be 3-5 sentences long, with incredible detail\n",
    "    9. Have clear emotional impact\n",
    "    \n",
    "    Format each prompt as:\n",
    "    \n",
    "    SCENE #: [Title]\n",
    "    [Detailed, evocative image prompt with all elements above]\n",
    "    \n",
    "    Examples of excellent prompts:\n",
    "    \"A close-up of a chubby fox face inside a hollow tree, eyes closed in thought, waiting patiently—symbolic of reflection and growth. Magical lighting filters through knotholes, illuminating the rustic forest background with golden rays. Vibrant fairytale woodland setting with moss and tiny mushrooms framing the scene. Expressive character design with detailed fur textures and a peaceful expression.\"\n",
    "    \n",
    "    \"A majestic eagle with sharp, piercing eyes and outstretched talons swoops down from a dark, stormy sky, its wings spread wide, casting a dramatic shadow on the ancient, moss-covered stones beneath, as it grasps a juicy, smoldering piece of meat. The flickering flame illuminates the intense, primal scene, surrounded by eerie, twisted trees with gnarled branches like withered fingers. Rendered in a vibrant, fairytale illustration style with rich, bold lines, intricate textures, and a muted, earthy color palette, evoking foreboding and symbolic greed.\"\n",
    "    \n",
    "    Create prompts that would produce a cohesive visual narrative across all images if generated in sequence.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440513b",
   "metadata": {},
   "source": [
    "## Create metadata class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d10906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import functools\n",
    "\n",
    "from datetime import datetime\n",
    "import tiktoken  # For token counting\n",
    "\n",
    "# Global metadata store\n",
    "metadata = {\n",
    "    \"session_start\": time.time(),\n",
    "    \"session_id\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "    \"current_story\": None,\n",
    "    \"stories\": {},\n",
    "    \"total_tokens\": 0,\n",
    "    \"total_cost\": 0,\n",
    "    \"total_time\": 0\n",
    "}\n",
    "\n",
    "def track_node(node_name, tool_name=\"main\"):\n",
    "    \"\"\"Decorator to track execution time of nodes\"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(state):\n",
    "            # Get current story ID or create one\n",
    "            story_id = metadata.get(\"current_story\")\n",
    "            if not story_id:\n",
    "                story_id = f\"story_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "                metadata[\"current_story\"] = story_id\n",
    "                metadata[\"stories\"][story_id] = {\n",
    "                    \"start_time\": time.time(),\n",
    "                    \"nodes\": {},\n",
    "                    \"tools\": {},\n",
    "                    \"llm_calls\": [],\n",
    "                    \"total_tokens\": 0,\n",
    "                    \"total_cost\": 0\n",
    "                }\n",
    "            \n",
    "            # Initialize node data if needed\n",
    "            story = metadata[\"stories\"][story_id]\n",
    "            if node_name not in story[\"nodes\"]:\n",
    "                story[\"nodes\"][node_name] = {\n",
    "                    \"calls\": 0,\n",
    "                    \"total_time\": 0,\n",
    "                    \"tokens\": 0\n",
    "                }\n",
    "            \n",
    "            # Start timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Call the function\n",
    "            try:\n",
    "                result = func(state)\n",
    "                # Record timing data\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                # Update metrics\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                \n",
    "                print(f\"Node {node_name} executed in {duration:.2f} seconds\")\n",
    "                \n",
    "                return result\n",
    "            except Exception as e:\n",
    "                # Record error but re-raise\n",
    "                end_time = time.time()\n",
    "                duration = end_time - start_time\n",
    "                \n",
    "                print(f\"Error in {node_name}: {str(e)}\")\n",
    "                story[\"nodes\"][node_name][\"calls\"] += 1\n",
    "                story[\"nodes\"][node_name][\"total_time\"] += duration\n",
    "                story[\"nodes\"][node_name][\"errors\"] = story[\"nodes\"][node_name].get(\"errors\", 0) + 1\n",
    "                \n",
    "                raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def track_llm_call(node_name, tool_name, model, system_prompt, user_prompt, response_text):\n",
    "    \"\"\"Track an LLM API call\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    # Simple token estimation (very rough)\n",
    "    input_tokens = (len(system_prompt) + len(user_prompt)) // 4  # ~4 chars per token\n",
    "    output_tokens = len(response_text) // 4\n",
    "    \n",
    "    # Cost estimation (very rough)\n",
    "    if model == \"gpt-4.1-mini\":\n",
    "        input_cost = (input_tokens / 1000) * 0.00040\n",
    "        output_cost = (output_tokens / 1000) * 0.0016\n",
    "    else:\n",
    "        input_cost = (input_tokens / 1000) * 0.00010\n",
    "        output_cost = (output_tokens / 1000) * 0.00030\n",
    "    \n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    # Record the call\n",
    "    call_data = {\n",
    "        \"timestamp\": time.time(),\n",
    "        \"node\": node_name,\n",
    "        \"tool\": tool_name,\n",
    "        \"model\": model,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": input_tokens + output_tokens,\n",
    "        \"total_cost\": total_cost\n",
    "    }\n",
    "    \n",
    "    # Update story\n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"llm_calls\"].append(call_data)\n",
    "    story[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    story[\"total_cost\"] += total_cost\n",
    "    \n",
    "    # Update node\n",
    "    if node_name in story[\"nodes\"]:\n",
    "        story[\"nodes\"][node_name][\"tokens\"] += input_tokens + output_tokens\n",
    "    \n",
    "    # Update global\n",
    "    metadata[\"total_tokens\"] += input_tokens + output_tokens\n",
    "    metadata[\"total_cost\"] += total_cost\n",
    "    \n",
    "    print(f\"LLM call in {node_name}: {input_tokens + output_tokens} tokens, ${total_cost:.4f}\")\n",
    "\n",
    "def finish_story(output_text):\n",
    "    \"\"\"Finish tracking the current story\"\"\"\n",
    "    story_id = metadata.get(\"current_story\")\n",
    "    if not story_id:\n",
    "        return\n",
    "    \n",
    "    story = metadata[\"stories\"][story_id]\n",
    "    story[\"Generated Sttory\"] = output_text\n",
    "    story[\"end_time\"] = time.time()\n",
    "    story[\"duration\"] = story[\"end_time\"] - story[\"start_time\"]\n",
    "    story[\"output_length\"] = len(output_text)\n",
    "    \n",
    "    # Calculate summary stats\n",
    "    total_time = sum(node[\"total_time\"] for node in story[\"nodes\"].values())\n",
    "    total_calls = sum(node[\"calls\"] for node in story[\"nodes\"].values())\n",
    "    \n",
    "    print(\"\\n=== STORY METRICS ===\")\n",
    "    print(f\"Total execution time: {story['duration']:.2f} seconds\")\n",
    "    print(f\"Total tokens: {story['total_tokens']} tokens\")\n",
    "    print(f\"Estimated cost: ${story['total_cost']:.4f}\")\n",
    "    print(f\"Number of nodes executed: {len(story['nodes'])}\")\n",
    "    print(f\"Number of LLM calls: {len(story['llm_calls'])}\")\n",
    "    \n",
    "    # Reset current story\n",
    "    metadata[\"current_story\"] = None\n",
    "    metadata[\"total_time\"] += story[\"duration\"]\n",
    "    \n",
    "    # Export metadata\n",
    "    with open(f\"story_metrics_{story_id}.json\", \"w\") as f:\n",
    "        json.dump(story, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"Metrics saved to story_metrics_{story_id}.json\")\n",
    "    \n",
    "    return story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c197d69b",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "Import OPENAI as a global variable and define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "373f8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## Call the api keys from the .env file\n",
    "llm_openai_41_mini = ChatOpenAI(model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3899b965",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "Define the the states of the main graph and the subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a7c1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainState(TypedDict):\n",
    "    messages: List[Dict[str, Any]]  # User messages\n",
    "    current_fable: str              # Input fable text\n",
    "    tool_to_call: str               # Which tool subgraph to use\n",
    "    processing_request: Dict        # Request info for the tool\n",
    "    tool_output: Dict               # Output from selected tool\n",
    "    final_story: str                # Final output after post-processing\n",
    "    image_prompts: List[Dict]       # List of image prompts for visualization\n",
    "\n",
    "# Cell 3: Define Aesop State (for subgraph)\n",
    "class AesopState(TypedDict):\n",
    "    original_fable: str             # Original input text\n",
    "    analysis: Dict                  # Analysis of the fable (moral, characters, etc)\n",
    "    brainstorm: Dict                # Ideas for the story creation/modification\n",
    "    generated_story: str            # The final story created by this tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f881a4a",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "Create the story router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ddea820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Main Agent for the main graph\n",
    "@track_node(\"main_agent\", \"main\")\n",
    "def main_agent(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main orchestrator that analyzes input and decides which tool to use\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Main Agent ===\")\n",
    "    current_fable = state.get(\"current_fable\", \"\")\n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    user_message = messages[-1].get(\"content\", \"\") if messages else \"\"\n",
    "    print(f\"Processing request: {user_message}\")\n",
    "    print(f\"Current fable length: {len(current_fable)} characters\")\n",
    "    \n",
    "    is_aesop = True\n",
    "    \n",
    "    if is_aesop:\n",
    "        processing_request = {\n",
    "            \"user_intent\": user_message,\n",
    "            \"fable_text\": current_fable\n",
    "        }\n",
    "        \n",
    "        result = {\n",
    "            \"processing_request\": processing_request,\n",
    "            \"tool_to_call\": \"aesop_tool\"\n",
    "        }\n",
    "        print(f\"Selecting tool: aesop_tool\")\n",
    "        return result\n",
    "    else:\n",
    "        # Future expansion for other tools\n",
    "        return {\n",
    "            \"processing_request\": {},\n",
    "            \"tool_to_call\": \"generic_tool\" \n",
    "        }\n",
    "\n",
    "# Cell 5: Tool Router\n",
    "def tool_router(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Routes to the appropriate tool subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Tool Router ===\")\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    processing_request = state.get(\"processing_request\", {})\n",
    "    \n",
    "    print(f\"Routing to tool: {tool_name}\")\n",
    "    \n",
    "    if tool_name == \"aesop_tool\":\n",
    "        # Call the Aesop subgraph\n",
    "        aesop_result = aesop_subgraph(processing_request)\n",
    "        print(f\"Received result from Aesop tool\")\n",
    "        return {\"tool_output\": aesop_result}\n",
    "    else:\n",
    "        # Future: add more tool subgraphs\n",
    "        return {\"tool_output\": {\"error\": \"Tool not found\"}}\n",
    "\n",
    "@track_node(\"generate_output\", \"main\")\n",
    "def generate_output(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Formats the final output based on the tool results\n",
    "    Creates a two-part story with strict word count limits\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Generate Output ===\")\n",
    "    tool_output = state.get(\"tool_output\", {})\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    \n",
    "    if \"error\" in tool_output:\n",
    "        final_story = f\"Error: {tool_output['error']}\"\n",
    "    else:\n",
    "        if tool_name == \"aesop_tool\":\n",
    "            # Get story components from the tool output\n",
    "            generated_story = tool_output.get(\"generated_story\", \"\")\n",
    "            analysis = tool_output.get(\"analysis\", {})\n",
    "            brainstorm = tool_output.get(\"brainstorm\", {})\n",
    "            \n",
    "            # Track the original story length for metadata\n",
    "            original_word_count = len(generated_story.split())\n",
    "            print(f\"Original story word count: {original_word_count}\")\n",
    "            \n",
    "            # Create a prompt to split and enhance the story with strict word counts\n",
    "            system_prompt = \"\"\"You are a master storyteller specializing in micro-fables for social media.\n",
    "            Take this fable and transform it into a two-part story with STRICT word count limits:\n",
    "            \n",
    "            PART 1 (First Post):\n",
    "            - EXACTLY 80-90 WORDS MAXIMUM\n",
    "            - Contains the setup, characters, and builds tension\n",
    "            - Ends at a compelling moment that makes readers curious for the conclusion\n",
    "            - Should be clearly labeled as \"PART 1\"\n",
    "            \n",
    "            PART 2 (Second Post):\n",
    "            - EXACTLY 80-90 WORDS MAXIMUM (including the recap)\n",
    "            - Begins with a brief 1-sentence recap of Part 1, and explicitly add at the begginig In Part 1\n",
    "            - Contains the resolution and moral/takeaway\n",
    "            - Ends with a memorable phrase that captures the moral\n",
    "            - Should be clearly labeled as \"PART 2\"\n",
    "            \n",
    "            The word counts are STRICT REQUIREMENTS - do not exceed 80 words for each part.\n",
    "            Count your words carefully before finalizing each part.\n",
    "            \"\"\"\n",
    "            \n",
    "            user_prompt = f\"\"\"Story: {generated_story}\n",
    "            \n",
    "            Analysis: {json.dumps(analysis, indent=2)}\n",
    "            \n",
    "            Split this into a two-part story as described, with exactly 80 words maximum for each part.\n",
    "            Ensure Part 2 begins with a brief recap so viewers can understand the conclusion even if they missed Part 1.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Call LLM to reformat the story\n",
    "            response = llm_openai_41_mini.invoke([\n",
    "                SystemMessage(content=system_prompt),\n",
    "                HumanMessage(content=user_prompt)\n",
    "            ])\n",
    "            \n",
    "            # Track LLM call\n",
    "            track_llm_call(\n",
    "                node_name=\"generate_output\",\n",
    "                tool_name=\"main\",\n",
    "                model=\"gpt-4.1-mini\",\n",
    "                system_prompt=system_prompt,\n",
    "                user_prompt=user_prompt,\n",
    "                response_text=response.content\n",
    "            )\n",
    "            \n",
    "            # The final reformatted story\n",
    "            final_story = response.content\n",
    "            \n",
    "            # Track final word count for metadata\n",
    "            final_word_count = len(final_story.split())\n",
    "            print(f\"Final story word count: {final_word_count}\")\n",
    "            \n",
    "            # Verify the word counts of each part (for validation and debugging)\n",
    "            parts = final_story.split(\"PART 2\")\n",
    "            if len(parts) > 1:\n",
    "                part1 = parts[0].replace(\"PART 1\", \"\").strip()\n",
    "                part2 = \"PART 2\" + parts[1].strip()\n",
    "                \n",
    "                part1_words = len(part1.split())\n",
    "                part2_words = len(part2.split())\n",
    "                \n",
    "                print(f\"Part 1 word count: {part1_words}\")\n",
    "                print(f\"Part 2 word count: {part2_words}\")\n",
    "                \n",
    "                # Add to metadata\n",
    "                if \"current_story\" in metadata and metadata[\"current_story\"] in metadata[\"stories\"]:\n",
    "                    story_id = metadata[\"current_story\"]\n",
    "                    if \"transformations\" not in metadata[\"stories\"][story_id]:\n",
    "                        metadata[\"stories\"][story_id][\"transformations\"] = {}\n",
    "                    \n",
    "                    metadata[\"stories\"][story_id][\"transformations\"][\"story_split\"] = {\n",
    "                        \"original_word_count\": original_word_count,\n",
    "                        \"final_word_count\": final_word_count,\n",
    "                        \"part1_word_count\": part1_words,\n",
    "                        \"part2_word_count\": part2_words,\n",
    "                        \"transformation_type\": \"two-part-fixed-length\",\n",
    "                        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }\n",
    "                    \n",
    "                    print(\"Added transformation metadata\")\n",
    "        else:\n",
    "            # Future: handle other tool outputs with different formatting strategies\n",
    "            final_story = str(tool_output)\n",
    "    \n",
    "    print(f\"Final story generated (length: {len(final_story)} characters)\")\n",
    "    return {\"final_story\": final_story}\n",
    "\n",
    "\n",
    "@track_node(\"image_prompt_generator\", \"main\")\n",
    "def image_prompt_generator(state: MainState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates image prompts for key scenes in the two-part story\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Image Prompt Generator ===\")\n",
    "    final_story = state.get(\"final_story\", \"\")\n",
    "    tool_output = state.get(\"tool_output\", {})\n",
    "    \n",
    "    # Extract analysis for context\n",
    "    analysis = tool_output.get(\"analysis\", {})\n",
    "    \n",
    "    # Split the story into parts\n",
    "    parts = final_story.split(\"PART 2\")\n",
    "    if len(parts) < 2:\n",
    "        parts = [final_story, \"\"]  # Fallback if not properly split\n",
    "    \n",
    "    part1 = parts[0].replace(\"PART 1\", \"\").strip()\n",
    "    part2 = \"PART 2\" + parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    \n",
    "    # Create prompt for generating image descriptions\n",
    "    system_prompt = IMAGE_PROMPT_GENERATOR_PROMPT\n",
    "    \n",
    "    user_prompt = f\"\"\"Two-part fable to visualize:\n",
    "\n",
    "    PART 1:\n",
    "    {part1}\n",
    "\n",
    "    PART 2:\n",
    "    {part2}\n",
    "\n",
    "    Character information from analysis:\n",
    "    {json.dumps(analysis.get('characters', {}), indent=2)}\n",
    "\n",
    "    Generate 8-10 image prompts for key visual moments throughout this story.\"\"\"\n",
    "    \n",
    "    # Call LLM to generate image prompts\n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ])\n",
    "    \n",
    "    # Track LLM call\n",
    "    track_llm_call(\n",
    "        node_name=\"image_prompt_generator\",\n",
    "        tool_name=\"main\",\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        response_text=response.content\n",
    "    )\n",
    "    \n",
    "    # Process the response into structured prompts\n",
    "    image_prompts_text = response.content\n",
    "    \n",
    "    # Parse the scene prompts\n",
    "    import re\n",
    "    scene_pattern = r'SCENE (\\d+): ([^\\n]+)\\n(.*?)(?=SCENE \\d+:|$)'\n",
    "    matches = re.findall(scene_pattern, image_prompts_text, re.DOTALL)\n",
    "    \n",
    "    image_prompts = []\n",
    "    for scene_num, title, description in matches:\n",
    "        image_prompts.append({\n",
    "            \"scene_number\": int(scene_num),\n",
    "            #\"title\": title.strip(),\n",
    "            \"description\": description.strip(),\n",
    "            \"story_part\": 1 if int(scene_num) <= len(matches)//2 else 2,  # Rough division between parts\n",
    "        })\n",
    "    \n",
    "    # Add metadata\n",
    "    if \"current_story\" in metadata and metadata[\"current_story\"] in metadata[\"stories\"]:\n",
    "        story_id = metadata[\"current_story\"]\n",
    "        metadata[\"stories\"][story_id][\"image_prompts\"] = {\n",
    "            \"count\": len(image_prompts),\n",
    "            \"prompts\": image_prompts,\n",
    "            \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "    \n",
    "    print(f\"Generated {len(image_prompts)} image prompts\")\n",
    "    \n",
    "    # Format a preview of prompts\n",
    "    preview = \"\\n\\n\".join([f\"SCENE {p['scene_number']}\" for p in image_prompts])\n",
    "    print(f\"Image prompt scenes:\\n{preview}\")\n",
    "    \n",
    "    # Return augmented state with image prompts\n",
    "    return {\"image_prompts\": image_prompts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e6013",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "Create the tools-subgraphs, currently we have:\n",
    "* Aesop tool-subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f8b4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node 1: Analyze Fable with tracking\n",
    "@track_node(\"analyze_fable\", \"aesop_tool\")\n",
    "def analyze_fable(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyzes the fable structure, characters, and moral\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Analyze Fable ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    print(f\"Analyzing fable (length: {len(original_fable)} characters)\")\n",
    "    \n",
    "    # Use LLM to analyze the fable with prompt from prompts.py\n",
    "    system_prompt = ANALYZE_FABLE_PROMPT\n",
    "    \n",
    "    user_prompt = f\"Analyze this fable:\\n\\n{original_fable}\"\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_prompt)\n",
    "    ])\n",
    "    \n",
    "    # Track the LLM call\n",
    "    track_llm_call(\n",
    "        node_name=\"analyze_fable\",\n",
    "        tool_name=\"aesop_tool\",\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        system_prompt=system_prompt,\n",
    "        user_prompt=user_prompt,\n",
    "        response_text=response.content\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        analysis = json.loads(response.content)\n",
    "    except:\n",
    "        # Fallback if JSON parsing fails\n",
    "        print(\"JSON parsing failed, using content as analysis\")\n",
    "        analysis = {\n",
    "            \"moral\": response.content,\n",
    "            \"characters\": [],\n",
    "            \"structure\": {},\n",
    "            \"symbols\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Analysis complete: identified moral '{analysis.get('moral', '')[:50]}...'\")\n",
    "    return {\"analysis\": analysis}\n",
    "\n",
    "@track_node(\"brainstorm_story\", \"aesop_tool\")\n",
    "# Node 2: Brainstorm Story\n",
    "def brainstorm_story(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Brainstorms ideas for story creation based on analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Brainstorm Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    \n",
    "    print(f\"Brainstorming based on analysis of moral: {analysis.get('moral', '')[:50]}...\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=BRAINSTORM_STORY_PROMPT),\n",
    "        HumanMessage(content=f\"Original fable: {original_fable}\\n\\nAnalysis: {json.dumps(analysis, indent=2)}\")\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        brainstorm = json.loads(response.content)\n",
    "    except:\n",
    "        print(\"JSON parsing failed, using content as brainstorm\")\n",
    "        brainstorm = {\n",
    "            \"moral_approaches\": response.content,\n",
    "            \"variations\": [],\n",
    "            \"character_ideas\": [],\n",
    "            \"imagery\": []\n",
    "        }\n",
    "    \n",
    "    print(f\"Brainstorming complete: generated {len(brainstorm.keys())} idea categories\")\n",
    "    return {\"brainstorm\": brainstorm}\n",
    "\n",
    "\n",
    "@track_node(\"generate_story_aessop\", \"aesop_tool\")\n",
    "# Node 3: Generate Story\n",
    "def generate_story_aessop(state: AesopState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates the final story based on analysis and brainstorming\n",
    "    \"\"\"\n",
    "    print(\"\\n== Aesop Subgraph: Generate Story ==\")\n",
    "    original_fable = state.get(\"original_fable\", \"\")\n",
    "    analysis = state.get(\"analysis\", {})\n",
    "    brainstorm = state.get(\"brainstorm\", {})\n",
    "    \n",
    "    print(f\"Generating story based on analysis and brainstorming\")\n",
    "    \n",
    "    response = llm_openai_41_mini.invoke([\n",
    "        SystemMessage(content=GENERATE_STORY_AESOP_PROMPT),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Original fable: {original_fable}\n",
    "        \n",
    "        Analysis: {json.dumps(analysis, indent=2)}\n",
    "        \n",
    "        Brainstorming: {json.dumps(brainstorm, indent=2)}\n",
    "        \n",
    "        Create a refined version of this fable.\n",
    "        \"\"\")\n",
    "    ])\n",
    "    \n",
    "    generated_story = response.content\n",
    "    print(f\"Story generated (length: {len(generated_story)} characters)\")\n",
    "    \n",
    "    return {\"generated_story\": generated_story}\n",
    "\n",
    "# Cell 8: BUILD THE AESOP SUBGRAPH\n",
    "def build_aesop_subgraph():\n",
    "    \"\"\"\n",
    "    Builds the Aesop tool subgraph\n",
    "    \"\"\"\n",
    "    builder = StateGraph(AesopState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"analyze_fable\", analyze_fable)\n",
    "    builder.add_node(\"brainstorm_story\", brainstorm_story)\n",
    "    builder.add_node(\"generate_story_aessop\", generate_story_aessop)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"analyze_fable\")\n",
    "    builder.add_edge(\"analyze_fable\", \"brainstorm_story\")\n",
    "    builder.add_edge(\"brainstorm_story\", \"generate_story_aessop\")\n",
    "    builder.add_edge(\"generate_story_aessop\", END)\n",
    "    \n",
    "    graph = builder.compile()\n",
    "\n",
    "    # # Generate the graph\n",
    "    # mermaid_graph = graph.get_graph().draw_mermaid_png(\n",
    "    #     draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "    #     output_file_path=\"./graphs_images/aesop_subgraph.png\"  # Specify where to save\n",
    "    # )\n",
    "\n",
    "    # Compile\n",
    "    return builder.compile()\n",
    "\n",
    "# Cell 9: AESOP SUBGRAPH WRAPPER\n",
    "def aesop_subgraph(processing_request: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Wrapper function that runs the Aesop subgraph\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Running Aesop Subgraph ===\")\n",
    "    \n",
    "    # Create initial state for Aesop subgraph\n",
    "    initial_state = {\n",
    "        \"original_fable\": processing_request.get(\"fable_text\", \"\"),\n",
    "        \"analysis\": {},\n",
    "        \"brainstorm\": {},\n",
    "        \"generated_story\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Build and run the subgraph\n",
    "    aesop_graph = build_aesop_subgraph()\n",
    "    result = aesop_graph.invoke(initial_state)\n",
    "    \n",
    "    # Return the enriched state\n",
    "    return {\n",
    "        \"analysis\": result[\"analysis\"],\n",
    "        \"brainstorm\": result[\"brainstorm\"],\n",
    "        \"generated_story\": result[\"generated_story\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e33a4e",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "Build the main graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aea35d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 10: BUILD THE MAIN GRAPH\n",
    "def decide_next_step(state: MainState) -> str:\n",
    "    \"\"\"\n",
    "    Decide whether to generate image prompts based on tool type\n",
    "    \"\"\"\n",
    "    tool_name = state.get(\"tool_to_call\", \"\")\n",
    "    \n",
    "    if tool_name == \"aesop_tool\":\n",
    "        return \"image_prompt_generator\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "def build_main_graph():\n",
    "    \"\"\"\n",
    "    Builds the main orchestrator graph with image prompt generation\n",
    "    \"\"\"\n",
    "    builder = StateGraph(MainState)\n",
    "    \n",
    "    # Add nodes\n",
    "    builder.add_node(\"main_agent\", main_agent)\n",
    "    builder.add_node(\"tool_router\", tool_router)\n",
    "    builder.add_node(\"generate_output\", generate_output)\n",
    "    builder.add_node(\"image_prompt_generator\", image_prompt_generator)\n",
    "    \n",
    "    # Add edges\n",
    "    builder.add_edge(START, \"main_agent\")\n",
    "    builder.add_edge(\"main_agent\", \"tool_router\")\n",
    "    builder.add_edge(\"tool_router\", \"generate_output\")\n",
    "    \n",
    "    # Conditional edge after generate_output\n",
    "    builder.add_conditional_edges(\n",
    "        \"generate_output\",\n",
    "        decide_next_step,\n",
    "        {\n",
    "            \"image_prompt_generator\": \"image_prompt_generator\",\n",
    "            END: END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Final edge\n",
    "    builder.add_edge(\"image_prompt_generator\", END)\n",
    "    \n",
    "    # Compile\n",
    "    graph = builder.compile()\n",
    "\n",
    "    # # Generate the graph\n",
    "    # mermaid_graph = graph.get_graph().draw_mermaid_png(\n",
    "    #     draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "    #     output_file_path=\"./graphs_images/main_graph.png\"  # Specify where to save\n",
    "    # )\n",
    "\n",
    "    return graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c676ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_system():\n",
    "    # Build the main graph\n",
    "    main_graph = build_main_graph()\n",
    "    print(\"Graph built successfully!\")\n",
    "    \n",
    "    # Test with a sample Aesop fable\n",
    "    test_fable = \"\"\"\n",
    "   La zorra y el perro.\n",
    "\n",
    "Penetró una zorra en un rebaño de corderos, y arrimando a su pecho\n",
    "a un pequeño corderillo, fingió acariciarle.\n",
    "\n",
    "Llegó un perro de los que cuidaban el rebaño y le preguntó:\n",
    "\n",
    "-- ¿Qué estás haciendo?\n",
    "\n",
    "-- Le acaricio y juego con él -- contestó con cara de inocencia.\n",
    "-- ¡ Pues suéltalo enseguida, si no quieres\n",
    "conocer mis mejores caricias!\n",
    "\n",
    "Al impreparado lo delatan sus actos.\n",
    "Estudia y aprende con gusto y tendrás\n",
    "\n",
    "éxito en tu vida.\n",
    "    \"\"\"\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Analyze and enhance this fable\"}],\n",
    "        \"current_fable\": test_fable,\n",
    "        \"tool_to_call\": \"\",\n",
    "        \"processing_request\": {},\n",
    "        \"tool_output\": {},\n",
    "        \"final_story\": \"\",\n",
    "        \"image_prompts\": []\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    result = main_graph.invoke(initial_state)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"\\n=== FINAL STORY ===\")\n",
    "    print(result[\"final_story\"])\n",
    "    \n",
    "    print(\"\\n=== IMAGE PROMPTS ===\")\n",
    "    for prompt in result.get(\"image_prompts\", []):\n",
    "        print(f\"\\nSCENE {prompt['scene_number']}\")\n",
    "        print(prompt['description'])\n",
    "    \n",
    "    # Finish tracking this story\n",
    "    story_stats = finish_story(result[\"final_story\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70848946",
   "metadata": {},
   "source": [
    "## Step 6:\n",
    "Test the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "436e07bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built successfully!\n",
      "\n",
      "=== Main Agent ===\n",
      "Processing request: Analyze and enhance this fable\n",
      "Current fable length: 463 characters\n",
      "Selecting tool: aesop_tool\n",
      "Node main_agent executed in 0.00 seconds\n",
      "\n",
      "=== Tool Router ===\n",
      "Routing to tool: aesop_tool\n",
      "\n",
      "=== Running Aesop Subgraph ===\n",
      "\n",
      "== Aesop Subgraph: Analyze Fable ==\n",
      "Analyzing fable (length: 463 characters)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Cell to actually run the test\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_result = \u001b[43mtest_system\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtest_system\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     27\u001b[39m initial_state = {\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAnalyze and enhance this fable\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcurrent_fable\u001b[39m\u001b[33m\"\u001b[39m: test_fable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage_prompts\u001b[39m\u001b[33m\"\u001b[39m: []\n\u001b[32m     35\u001b[39m }\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m result = \u001b[43mmain_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== FINAL STORY ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2823\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2820\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2821\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2823\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2835\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2836\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2837\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/runner.py:153\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    151\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mtool_router\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRouting to tool: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool_name == \u001b[33m\"\u001b[39m\u001b[33maesop_tool\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Call the Aesop subgraph\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     aesop_result = \u001b[43maesop_subgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessing_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived result from Aesop tool\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mtool_output\u001b[39m\u001b[33m\"\u001b[39m: aesop_result}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36maesop_subgraph\u001b[39m\u001b[34m(processing_request)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Build and run the subgraph\u001b[39;00m\n\u001b[32m    154\u001b[39m aesop_graph = build_aesop_subgraph()\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m result = \u001b[43maesop_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Return the enriched state\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    159\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33manalysis\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33manalysis\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    160\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbrainstorm\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mbrainstorm\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    161\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgenerated_story\u001b[39m\u001b[33m\"\u001b[39m: result[\u001b[33m\"\u001b[39m\u001b[33mgenerated_story\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    162\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2823\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2820\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2821\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2823\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2835\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2836\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2837\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/runner.py:153\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    151\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/pregel/retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/utils/runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langgraph/utils/runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtrack_node.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     \u001b[38;5;66;03m# Record timing data\u001b[39;00m\n\u001b[32m     54\u001b[39m     end_time = time.time()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36manalyze_fable\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     12\u001b[39m system_prompt = ANALYZE_FABLE_PROMPT\n\u001b[32m     14\u001b[39m user_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnalyze this fable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00moriginal_fable\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mllm_openai_41_mini\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Track the LLM call\u001b[39;00m\n\u001b[32m     22\u001b[39m track_llm_call(\n\u001b[32m     23\u001b[39m     node_name=\u001b[33m\"\u001b[39m\u001b[33manalyze_fable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     tool_name=\u001b[33m\"\u001b[39m\u001b[33maesop_tool\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     response_text=response.content\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:370\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m     **kwargs: Any,\n\u001b[32m    366\u001b[39m ) -> BaseMessage:\n\u001b[32m    367\u001b[39m     config = ensure_config(config)\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    369\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    380\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:947\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    940\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     **kwargs: Any,\n\u001b[32m    945\u001b[39m ) -> LLMResult:\n\u001b[32m    946\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:766\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1012\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:959\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/openai/_base_client.py:969\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    967\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    975\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/StoryForge/storyforge/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell to actually run the test\n",
    "test_result = test_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2b99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ece30471",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "client = genai.Client(api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single story format detected\n",
      "  - 10 prompts available\n",
      "Loading story data from: story_metrics_story_16_20250606_202857.json\n",
      "Using flat structure for story: story_metrics_story_16_20250606_202857\n",
      "Found 10 image prompts to generate\n",
      "\n",
      "Generating Scene 1: Neon Reef City’s Electric Surge\n",
      "Part 1 | Prompt preview: Beneath a vast, glowing underwater city, sleek electric eels writhe through neon-lit waterways, thei...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_01_Neon Reef Citys Electric Surge_v1.png\n",
      "\n",
      "Generating Scene 2: The Coral Polyps’ Plea\n",
      "Part 1 | Prompt preview: A group of delicate, translucent coral polyps stretch their fragile tendrils upward, eyes wide with ...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_02_The Coral Polyps Plea_v1.png\n",
      "\n",
      "Generating Scene 3: The Wise Shadowy Octopuses\n",
      "Part 1 | Prompt preview: In the deep recesses of a dark coral cavern, silent and fluid octopuses emerge, their inky bodies sh...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_03_The Wise Shadowy Octopuses_v1.png\n",
      "\n",
      "Generating Scene 4: The Octopuses’ Demand for Clarity\n",
      "Part 1 | Prompt preview: A close scene of an octopus raising an arm in a gentle, almost diplomatic gesture, while in the wate...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_04_The Octopuses Demand for Clarity_v1.png\n",
      "\n",
      "Generating Scene 5: The Octopuses Vanish into the Depths\n",
      "Part 1 | Prompt preview: In a swirling spectacle of jet black ink spirals and fading bioluminescence, the wise octopuses sile...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_05_The Octopuses Vanish into the Depths_v1.png\n",
      "\n",
      "Generating Scene 6: The Polyps’ Moment of Pausing in Doubt\n",
      "Part 2 | Prompt preview: A tender, intimate close-up on a cluster of coral polyps whose gentle glows flicker weakly in dimini...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_06_The Polyps Moment of Pausing in Doubt_v1.png\n",
      "\n",
      "Generating Scene 7: Observing the Currents, Learning Clarity\n",
      "Part 2 | Prompt preview: The coral polyps are depicted gently extending luminous tendrils into slow-moving, clear currents, t...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_07_Observing the Currents Learning Clarity_v1.png\n",
      "\n",
      "Generating Scene 8: The Storm’s Quiet Descent\n",
      "Part 2 | Prompt preview: In a wide, tranquil tableau of the reef-city now bathed in gentle twilight, the storm’s ominous hum ...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_08_The Storms Quiet Descent_v1.png\n",
      "\n",
      "Generating Scene 9: Illuminated Path Through the Storm\n",
      "Part 2 | Prompt preview: A symbolic, metaphorical scene shows a winding pathway of glowing coral tendrils and bioluminescent ...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_09_Illuminated Path Through the Storm_v1.png\n",
      "\n",
      "Generating Scene 10: The Moral Embodied\n",
      "Part 2 | Prompt preview: A final, poignant image shows a wise octopus gently resting one arm on a glowing coral polyp, both i...\n",
      "  ✓ Overwritten: generated_images/story_metrics_story_16_20250606_202857/scene_10_The Moral Embodied_v1.png\n",
      "\n",
      "🎉 Image generation complete!\n",
      "📁 Images saved to: generated_images/story_metrics_story_16_20250606_202857\n",
      "📊 Summary:\n",
      "   - Generated: 10 images\n",
      "   - Skipped: 0 images\n",
      "   - Errors: 0 scenes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "\n",
    "def load_story_metadata(json_file_path):\n",
    "    \"\"\"Load the story metadata from JSON file\"\"\"\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def generate_images_for_story(json_file_path, output_dir=\"generated_images\", overwrite=False):\n",
    "    \"\"\"\n",
    "    Generate images from the JSON file for any story structure\n",
    "    \n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file\n",
    "        output_dir: Directory to save images\n",
    "        overwrite: If True, overwrites existing images. If False, skips existing files.\n",
    "    \"\"\"\n",
    "    print(f\"Loading story data from: {json_file_path}\")\n",
    "    \n",
    "    # # Setup - you'll need to set your API key\n",
    "    # GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    # if not GEMINI_API_KEY:\n",
    "    #     raise ValueError(\"Please set GOOGLE_API_KEY in your environment variables\")\n",
    "    \n",
    "    client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    # Load the story data\n",
    "    story_data = load_story_metadata(json_file_path)\n",
    "    \n",
    "    # MADE GENERAL: Handle different JSON structures\n",
    "    prompts_data = None\n",
    "    story_id = None\n",
    "    \n",
    "    # Check if this is the new structure with 'stories' key\n",
    "    if \"stories\" in story_data:\n",
    "        # Get the first (and likely only) story\n",
    "        stories = story_data[\"stories\"]\n",
    "        if not stories:\n",
    "            print(\"No stories found in the JSON file\")\n",
    "            return\n",
    "        \n",
    "        # Get the first story ID\n",
    "        story_id = list(stories.keys())[0]\n",
    "        story_info = stories[story_id]\n",
    "        \n",
    "        if \"image_prompts\" in story_info:\n",
    "            prompts_data = story_info[\"image_prompts\"]\n",
    "            print(f\"Found story: {story_id}\")\n",
    "    \n",
    "    # Check if this is the old flat structure\n",
    "    elif \"image_prompts\" in story_data:\n",
    "        prompts_data = story_data[\"image_prompts\"]\n",
    "        story_id = Path(json_file_path).stem  # Use filename as story ID\n",
    "        print(f\"Using flat structure for story: {story_id}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"No image prompts found in the JSON file\")\n",
    "        return\n",
    "    \n",
    "    prompts = prompts_data.get(\"prompts\", [])\n",
    "    \n",
    "    if not prompts:\n",
    "        print(\"No prompts found in image_prompts\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {len(prompts)} image prompts to generate\")\n",
    "    \n",
    "    # Create output directory\n",
    "    story_dir = Path(output_dir) / story_id\n",
    "    story_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Track statistics\n",
    "    generated_count = 0\n",
    "    skipped_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Generate images for each prompt\n",
    "    for i, prompt_data in enumerate(prompts):\n",
    "        scene_number = prompt_data.get(\"scene_number\", i+1)\n",
    "        #title = prompt_data.get(\"title\", f\"Scene {scene_number}\")\n",
    "        prompt_text = prompt_data.get(\"description\", \"\")\n",
    "        story_part = prompt_data.get(\"story_part\", 1)\n",
    "        \n",
    "        print(f\"\\nGenerating Scene {scene_number}\")\n",
    "        print(f\"Part {story_part} | Prompt preview: {prompt_text[:100]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate images using Imagen 3\n",
    "            response = client.models.generate_images(\n",
    "                model='imagen-3.0-generate-002',\n",
    "                prompt=prompt_text,\n",
    "                config=types.GenerateImagesConfig(\n",
    "                    number_of_images=1,\n",
    "                    aspectRatio=\"9:16\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Save the generated images\n",
    "            for img_idx, generated_image in enumerate(response.generated_images):\n",
    "                # Create filename\n",
    "                #safe_title = \"\".join(c for c in title if c.isalnum() or c in (' ', '-', '_')).rstrip()\n",
    "                filename = f\"scene_{scene_number:02d}_v{img_idx+1}.png\"\n",
    "                filepath = story_dir / filename\n",
    "                \n",
    "                # CHECK IF FILE EXISTS AND HANDLE OVERWRITE\n",
    "                if filepath.exists() and not overwrite:\n",
    "                    print(f\"  ⏭ Skipped (already exists): {filepath}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Save the image\n",
    "                generated_image.image.save(str(filepath))\n",
    "                \n",
    "                if filepath.exists() and not overwrite:\n",
    "                    print(f\"  ✓ Generated: {filepath}\")\n",
    "                else:\n",
    "                    print(f\"  ✓ Overwritten: {filepath}\")\n",
    "                generated_count += 1\n",
    "            \n",
    "            # Add a small delay to avoid rate limiting\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error generating image for Scene {scene_number}: {str(e)}\")\n",
    "            error_count += 1\n",
    "            continue\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n🎉 Image generation complete!\")\n",
    "    print(f\"📁 Images saved to: {story_dir}\")\n",
    "    print(f\"📊 Summary:\")\n",
    "    print(f\"   - Generated: {generated_count} images\")\n",
    "    print(f\"   - Skipped: {skipped_count} images\")\n",
    "    print(f\"   - Errors: {error_count} scenes\")\n",
    "\n",
    "def list_stories_in_json(json_file_path):\n",
    "    \"\"\"\n",
    "    List all stories available in the JSON file\n",
    "    \"\"\"\n",
    "    story_data = load_story_metadata(json_file_path)\n",
    "    \n",
    "    if \"stories\" in story_data:\n",
    "        stories = story_data[\"stories\"]\n",
    "        print(f\"Found {len(stories)} story(ies):\")\n",
    "        for story_id, story_info in stories.items():\n",
    "            prompt_count = story_info.get(\"image_prompts\", {}).get(\"count\", 0)\n",
    "            duration = story_info.get(\"duration\", 0)\n",
    "            print(f\"  - {story_id}: {prompt_count} prompts, {duration:.1f}s duration\")\n",
    "    else:\n",
    "        print(\"Single story format detected\")\n",
    "        prompt_count = story_data.get(\"image_prompts\", {}).get(\"count\", 0)\n",
    "        print(f\"  - {prompt_count} prompts available\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # List what stories are available\n",
    "    list_stories_in_json(\"story_metrics_story_16_20250606_202857.json\")\n",
    "    \n",
    "    # Generate images (won't overwrite by default)\n",
    "    generate_images_for_story(\"story_metrics_story_16_20250606_202857.json\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b877a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storyforge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
